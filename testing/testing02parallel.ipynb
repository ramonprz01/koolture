{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Culture Measures Based on Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ramon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ramon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/ramon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewID</th>\n",
       "      <th>employerID</th>\n",
       "      <th>userID</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthYear</th>\n",
       "      <th>highestEducation</th>\n",
       "      <th>metroID</th>\n",
       "      <th>metroName</th>\n",
       "      <th>stateID</th>\n",
       "      <th>stateName</th>\n",
       "      <th>countryID</th>\n",
       "      <th>jobTitleID</th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>GOC</th>\n",
       "      <th>GOCconfidence</th>\n",
       "      <th>MGOC</th>\n",
       "      <th>MGOCconfidence</th>\n",
       "      <th>reviewDateTime</th>\n",
       "      <th>isCurrentJobFlag</th>\n",
       "      <th>jobEndingYear</th>\n",
       "      <th>OverallRating</th>\n",
       "      <th>CareerOpps</th>\n",
       "      <th>CompensationBenefits</th>\n",
       "      <th>SeniorLeadership</th>\n",
       "      <th>Worklife</th>\n",
       "      <th>CultureValues</th>\n",
       "      <th>RecommendFriend</th>\n",
       "      <th>BusinessOutlook</th>\n",
       "      <th>CEO</th>\n",
       "      <th>employerName</th>\n",
       "      <th>stockTicker</th>\n",
       "      <th>employerTypeCode</th>\n",
       "      <th>numberEmployees</th>\n",
       "      <th>annualRevenue</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4151950</td>\n",
       "      <td>11891</td>\n",
       "      <td>24353329</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>BACHELORS</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-30 23:52:26.027</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>YES</td>\n",
       "      <td>Same</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>COMPANY_PUBLIC</td>\n",
       "      <td>4700</td>\n",
       "      <td>8830669000</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>You will be working with the most talented ppl...</td>\n",
       "      <td>Little bit politics in some teams.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1863</td>\n",
       "      <td>11891</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>761</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>2280</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>35739</td>\n",
       "      <td>Director, Product Management</td>\n",
       "      <td>product manager</td>\n",
       "      <td>0.913</td>\n",
       "      <td>product manager</td>\n",
       "      <td>0.913</td>\n",
       "      <td>2008-04-23 23:42:17.157</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>COMPANY_PUBLIC</td>\n",
       "      <td>4700</td>\n",
       "      <td>8830669000</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Freedom and responsibility. You're treated lik...</td>\n",
       "      <td>Netflix is not for everyone. You don't get \"di...</td>\n",
       "      <td>I have none. Senior management is fantastic. s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4991</td>\n",
       "      <td>11891</td>\n",
       "      <td>2076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>761</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>2280</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>13321</td>\n",
       "      <td>Marketing Manager</td>\n",
       "      <td>marketing manager</td>\n",
       "      <td>1.000</td>\n",
       "      <td>marketing manager</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2008-06-11 00:03:28.907</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>COMPANY_PUBLIC</td>\n",
       "      <td>4700</td>\n",
       "      <td>8830669000</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Great colleagues -- incredible really</td>\n",
       "      <td>Domestic not global business -- wish we did eu...</td>\n",
       "      <td>Focus on the customer, not on Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53799</td>\n",
       "      <td>11891</td>\n",
       "      <td>68043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>Portland</td>\n",
       "      <td>3163</td>\n",
       "      <td>OR</td>\n",
       "      <td>1</td>\n",
       "      <td>64668</td>\n",
       "      <td>Support Staff</td>\n",
       "      <td>support staff</td>\n",
       "      <td>1.000</td>\n",
       "      <td>retail representative</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2008-08-07 23:30:14.267</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>COMPANY_PUBLIC</td>\n",
       "      <td>4700</td>\n",
       "      <td>8830669000</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>The upper management of Netflix really does se...</td>\n",
       "      <td>Specific to the Hillsboro location, the middle...</td>\n",
       "      <td>To the senior-most management in Los Gatos, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53937</td>\n",
       "      <td>11891</td>\n",
       "      <td>68207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>36451</td>\n",
       "      <td>Does IT Matter?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2008-08-08 09:12:42.493</td>\n",
       "      <td>0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approve</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>NFLX</td>\n",
       "      <td>COMPANY_PUBLIC</td>\n",
       "      <td>4700</td>\n",
       "      <td>8830669000</td>\n",
       "      <td>Internet</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>The people there are fantastic, the service is...</td>\n",
       "      <td>It's frustrating to work for direct management...</td>\n",
       "      <td>Stop being so secretive, just be upfront and h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewID  employerID    userID  gender  birthYear highestEducation  \\\n",
       "0   4151950       11891  24353329  FEMALE     1984.0        BACHELORS   \n",
       "1      1863       11891        -1     NaN        NaN              NaN   \n",
       "2      4991       11891      2076     NaN        NaN              NaN   \n",
       "3     53799       11891     68043     NaN        NaN              NaN   \n",
       "4     53937       11891     68207     NaN        NaN              NaN   \n",
       "\n",
       "   metroID metroName  stateID stateName  countryID  jobTitleID  \\\n",
       "0        0       NaN        0       NaN          1           0   \n",
       "1      761  San Jose     2280        CA          1       35739   \n",
       "2      761  San Jose     2280        CA          1       13321   \n",
       "3      700  Portland     3163        OR          1       64668   \n",
       "4        0       NaN        0       NaN          1       36451   \n",
       "\n",
       "                       JobTitle                GOC  GOCconfidence  \\\n",
       "0                           NaN                NaN            NaN   \n",
       "1  Director, Product Management    product manager          0.913   \n",
       "2             Marketing Manager  marketing manager          1.000   \n",
       "3                 Support Staff      support staff          1.000   \n",
       "4               Does IT Matter?                NaN          0.000   \n",
       "\n",
       "                    MGOC  MGOCconfidence           reviewDateTime  \\\n",
       "0                    NaN             NaN  2014-04-30 23:52:26.027   \n",
       "1        product manager           0.913  2008-04-23 23:42:17.157   \n",
       "2      marketing manager           1.000  2008-06-11 00:03:28.907   \n",
       "3  retail representative           1.000  2008-08-07 23:30:14.267   \n",
       "4                    NaN           0.000  2008-08-08 09:12:42.493   \n",
       "\n",
       "   isCurrentJobFlag  jobEndingYear  OverallRating  CareerOpps  \\\n",
       "0                 1            NaN            4.0         3.0   \n",
       "1                 1            NaN            5.0         4.0   \n",
       "2                 1            NaN            5.0         5.0   \n",
       "3                 0         2008.0            2.0         1.0   \n",
       "4                 0         2008.0            2.0         2.0   \n",
       "\n",
       "   CompensationBenefits  SeniorLeadership  Worklife  CultureValues  \\\n",
       "0                   5.0               3.0       2.0            3.0   \n",
       "1                   4.5               5.0       4.5            NaN   \n",
       "2                   5.0               5.0       4.5            NaN   \n",
       "3                   4.5               4.0       5.0            NaN   \n",
       "4                   2.5               3.5       1.0            NaN   \n",
       "\n",
       "  RecommendFriend BusinessOutlook      CEO employerName stockTicker  \\\n",
       "0             YES            Same  Approve      Netflix        NFLX   \n",
       "1             YES             NaN  Approve      Netflix        NFLX   \n",
       "2             YES             NaN  Approve      Netflix        NFLX   \n",
       "3              NO             NaN  Approve      Netflix        NFLX   \n",
       "4              NO             NaN  Approve      Netflix        NFLX   \n",
       "\n",
       "  employerTypeCode  numberEmployees  annualRevenue  industry  \\\n",
       "0   COMPANY_PUBLIC             4700     8830669000  Internet   \n",
       "1   COMPANY_PUBLIC             4700     8830669000  Internet   \n",
       "2   COMPANY_PUBLIC             4700     8830669000  Internet   \n",
       "3   COMPANY_PUBLIC             4700     8830669000  Internet   \n",
       "4   COMPANY_PUBLIC             4700     8830669000  Internet   \n",
       "\n",
       "                   sector                                               pros  \\\n",
       "0  Information Technology  You will be working with the most talented ppl...   \n",
       "1  Information Technology  Freedom and responsibility. You're treated lik...   \n",
       "2  Information Technology              Great colleagues -- incredible really   \n",
       "3  Information Technology  The upper management of Netflix really does se...   \n",
       "4  Information Technology  The people there are fantastic, the service is...   \n",
       "\n",
       "                                                cons  \\\n",
       "0                 Little bit politics in some teams.   \n",
       "1  Netflix is not for everyone. You don't get \"di...   \n",
       "2  Domestic not global business -- wish we did eu...   \n",
       "3  Specific to the Hillsboro location, the middle...   \n",
       "4  It's frustrating to work for direct management...   \n",
       "\n",
       "                                            feedback  \n",
       "0                                                NaN  \n",
       "1  I have none. Senior management is fantastic. s...  \n",
       "2                Focus on the customer, not on Apple  \n",
       "3  To the senior-most management in Los Gatos, I ...  \n",
       "4  Stop being so secretive, just be upfront and h...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('netflix.csv', parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2014\n",
       "1    2008\n",
       "2    2008\n",
       "3    2008\n",
       "4    2008\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewDateTime'] = pd.to_datetime(df['reviewDateTime'])\n",
    "df['year'] = df['reviewDateTime'].dt.year\n",
    "df['year'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will remove the company names from their respective reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def comp_name_out(data, col_to_search, col_reviews, companies):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, the name of the column with all of \n",
    "    the companies, the name of the column with the reviews, and an iterable\n",
    "    with the companies names that are in the dataset. The latter could be a list,\n",
    "    set, Series, tuple, etc.\n",
    "    \"\"\"\n",
    "    for company in companies:\n",
    "        condition = (data[col_to_search] == company)\n",
    "        data.loc[condition, col_reviews] = data.loc[condition, col_reviews].str.lower().str.replace(company.lower(), '', regex=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the root of the word. You can get all three (lemma, stem, and snow) or use them separately with the partial functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_of_word(docs, root_word_method='lemma'):\n",
    "    porter_stemmer = nltk.stem.PorterStemmer()\n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(docs)\n",
    "    \n",
    "    if root_word_method == 'lemma':\n",
    "        doc = ' '.join([lemma.lemmatize(w) for w in tokens])\n",
    "    elif root_word_method == 'stemm':\n",
    "        doc = ' '.join([porter_stemmer.stem(w) for w in tokens])\n",
    "    elif root_word_method == 'snowball':\n",
    "        doc = ' '.join([snowball_stemmer.stem(w) for w in tokens])\n",
    "        \n",
    "    return doc\n",
    "\n",
    "stemming = partial(root_of_word, root_word_method='stemm')\n",
    "snowball = partial(root_of_word, root_word_method='snowball')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps with the preprocessing of the data. It runs after the lemmatizer, stemmer, snowball, etc. If you want to include stopwords and take them out at a later stage, uncomment the first `filtered_tokens` below and comment out the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "#     filtered_tokens = [token for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to get the top topics and to run the LDA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words=15):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300\n",
    "\n",
    "def get_models(topics, tf, tup_num):\n",
    "    \"\"\"\n",
    "    This functions takes in the number of topics to run the model for,\n",
    "    a tuple with the name of the company and the sparse matix and\n",
    "    a number for the element in the tuple that has the sparse matix.\n",
    "    It then returns a tuple with (company name, topic #, comph, and the model)\n",
    "    \"\"\"\n",
    "    lda = LatentDirichletAllocation(n_components=topics, max_iter=100, learning_method='online', learning_offset=10., random_state=1234)\n",
    "    lda_model = lda.fit(tf[tup_num])\n",
    "    topicsOverWords = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    return (tf[0], topics, comph(topicsOverWords), lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(p, q, base=np.e): # JS distance between probability vectors, used to compute compH\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = (1 / 2 * (p + q))\n",
    "    return sp.stats.entropy(p, m, base) / 2 +  sp.stats.entropy(q, m, base) / 2\n",
    "\n",
    "def conth(data): # function to measure content heterogeneity given a topic (prob) matrix\n",
    "    return (1 / ((sum(map(sum, np.square(data.values)))) / data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comph(data): \n",
    "    #Transform probMatrix_df into 2D array\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    for x in range(len(data)): \n",
    "        jsd_list = []\n",
    "        for y in range(len(data)): \n",
    "            jsd_list.append(jsd(data[x], data[y]))\n",
    "        df[str(x)] = jsd_list\n",
    "\n",
    "    #Get df lower diagonal\n",
    "    mask = np.ones(df.shape, dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1) & mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    for k in range(len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(k)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [l for l in column_list if (math.isnan(l) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "            \n",
    "    return sum(distance_list) / float(len(distance_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array with the unique employers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2014, 2008, 2009, 2010, 2011, 2012, 2013, 2015, 2016, 2017, 2018])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = df['employerName'].unique()\n",
    "years = df['year'].unique()\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the company names from the reviews, and extract the reviews into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 642 Âµs, total: 22.1 ms\n",
      "Wall time: 22.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = comp_name_out(df, 'employerName', 'pros', companies)\n",
    "data_pros = df['pros'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel. You first normalize the reviews and then take the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 248 ms, total: 1.4 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(stemming, data_pros_cleaned))\n",
    "\n",
    "df['pros_clean'] = data_pros_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you create an array with all of the companies and the amount of reviews they have. So far, only companies with at least 2 reviews make it to the modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Netflix_2014\n",
       "1    Netflix_2008\n",
       "2    Netflix_2008\n",
       "3    Netflix_2008\n",
       "4    Netflix_2008\n",
       "Name: emp_year, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emp_year'] = df['employerName'] + '_' + df['year'].astype(str)\n",
    "df['emp_year'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " Index(['Netflix_2015', 'Netflix_2017', 'Netflix_2016', 'Netflix_2014',\n",
       "        'Netflix_2009', 'Netflix_2013', 'Netflix_2008', 'Netflix_2012',\n",
       "        'Netflix_2010', 'Netflix_2011', 'Netflix_2018'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps_of_interest = df['emp_year'].value_counts()\n",
    "comps_of_interest = (comps_of_interest).index\n",
    "len(comps_of_interest), comps_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the employers that meet the condition above by creating a boolean with True for yes and False for no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Netflix_2014', 'Netflix_2008', 'Netflix_2009', 'Netflix_2010',\n",
       "       'Netflix_2011', 'Netflix_2012', 'Netflix_2013', 'Netflix_2015',\n",
       "       'Netflix_2016', 'Netflix_2017', 'Netflix_2018'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond2 = df['emp_year'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['emp_year'].unique() # get the unique IDs or unique employers in the dataset\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will create sparse matrices for all companies and return a list of tuples with the name of the company, its sparse matrix, and the fitted vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 222 ms, sys: 0 ns, total: 222 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers_list = []\n",
    "for comp_id in unique_ids:\n",
    "    cond = (df_interest['emp_year'] == comp_id) # condition to get a dataset for each company\n",
    "    revs_clean = df_interest.loc[cond, 'pros_clean'].values # get an array of reviews for such company\n",
    "    count_vect = CountVectorizer() # instantiate a vectorizer\n",
    "    vect = count_vect.fit_transform(revs_clean) # fit it to the selected company reviews\n",
    "    vectorizers_list.append((comp_id, vect, count_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total words in the dictionary of review words, and get the percentage of words in the final dictionary that can be found in the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalWords_vectorizer = CountVectorizer()\n",
    "# TotalWords_tf = TotalWords_vectorizer.fit_transform(data_pros)\n",
    "# totWords = len(TotalWords_vectorizer.get_feature_names())\n",
    "# tf_vectorizer = CountVectorizer(max_df = 0.90, min_df=0.01)\n",
    "# tf = tf_vectorizer.fit_transform(data_pros_cleaned)\n",
    "# tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# percVoc = len(tf_feature_names) / totWords * 100\n",
    "# percVoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the models in parallel and return a dictionary with the output of the get_models function for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 383 ms, sys: 590 ms, total: 973 ms\n",
      "Wall time: 8min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bad_output = {}\n",
    "output_dictionary = {} # dictionary for the output\n",
    "\n",
    "for sparse_tup in vectorizers_list:\n",
    "    # to run get models in parallel, some parameters have to be fixed into a new function\n",
    "    # so we do that here with partial\n",
    "    partial_func = partial(get_models, tf=sparse_tup, tup_num=1)\n",
    "#     try:\n",
    "        # you will run several topics at a time for each company\n",
    "    with cf.ProcessPoolExecutor() as e:\n",
    "        output = list(e.map(partial_func, our_range))\n",
    "    output_dictionary[sparse_tup[0]] = output\n",
    "#     except:\n",
    "#         bad_tup = vectorizers_list.pop(vectorizers_list.index(sparse_tup))\n",
    "#         bad_output[bad_tup[0]] = bad_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(bad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now iterate over the output from above, add each dataset into a list, and then concatenate them all into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 ms, sys: 4.3 ms, total: 43.3 ms\n",
      "Wall time: 49.5 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>2</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>10</td>\n",
       "      <td>0.243416</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>50</td>\n",
       "      <td>0.298834</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>100</td>\n",
       "      <td>0.251752</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>150</td>\n",
       "      <td>0.199517</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>200</td>\n",
       "      <td>0.173241</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>250</td>\n",
       "      <td>0.142165</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>300</td>\n",
       "      <td>0.132144</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>2</td>\n",
       "      <td>0.103315</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>10</td>\n",
       "      <td>0.191917</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        company  topics  coherence  \\\n",
       "0  Netflix_2014       2   0.128400   \n",
       "1  Netflix_2014      10   0.243416   \n",
       "2  Netflix_2014      50   0.298834   \n",
       "3  Netflix_2014     100   0.251752   \n",
       "4  Netflix_2014     150   0.199517   \n",
       "5  Netflix_2014     200   0.173241   \n",
       "6  Netflix_2014     250   0.142165   \n",
       "7  Netflix_2014     300   0.132144   \n",
       "0  Netflix_2008       2   0.103315   \n",
       "1  Netflix_2008      10   0.191917   \n",
       "\n",
       "                                              models  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  \n",
       "6  LatentDirichletAllocation(learning_method='onl...  \n",
       "7  LatentDirichletAllocation(learning_method='onl...  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_list = []\n",
    "for data in output_dictionary.keys(): # you can use the keys to get the data\n",
    "    temp_df = pd.DataFrame.from_dict(output_dictionary[data])\n",
    "    dfs_list.append(temp_df)\n",
    "\n",
    "output_dfs = pd.concat(dfs_list)\n",
    "output_dfs.columns = ['company', 'topics', 'coherence', 'models']\n",
    "output_dfs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates over the new dataframe, searches for the top 2 topics based on highest coherence, and appends to a list a tuple containing the company, a tuple with the top two topic numbers, and the fitted vectorizer from the original `vectorizers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_topics = []\n",
    "for comp, tup in zip(unique_ids, vectorizers_list):\n",
    "    condition = output_dfs['company'] == comp # get each company\n",
    "    the_data = output_dfs[condition] # get an exclusive dataset for a company\n",
    "    top_condition = the_data['coherence'].argsort() # get a sorted index based on coherence\n",
    "    top_topics = the_data.loc[top_condition, 'topics'].values # get the sorted topics based on coherence\n",
    "    sorted_topics.append((comp, (top_topics[-2], top_topics[-1]), tup[1])) # put all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `get_models` function again over the new space of topics. You will  need to\n",
    "1. sort the tuple with the top two topics.\n",
    "2. create a linearly spaced array with 10 elements between the top 2 topics, turn it into integers, make the array a set to eliminate any duplicates that might arise if there is a 2 in the top two topics, and then turn that into a list.\n",
    "3. get your fixed partial function again\n",
    "4. the output is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 274 ms, sys: 547 ms, total: 821 ms\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_dictionary2 = {}\n",
    "\n",
    "for tup in sorted_topics:\n",
    "    start, end = sorted(tup[1]) # since the top 2 topics might not be sorted, sort them first\n",
    "    the_range = list(set(np.linspace(start, end, 10).astype(int))) \n",
    "    partial_func = partial(get_models, tf=tup, tup_num=2)\n",
    "    with cf.ProcessPoolExecutor() as e:\n",
    "        output = list(e.map(partial_func, the_range))\n",
    "    output_dictionary2[tup[0]] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple dataframes from dictionaries again and collapse them into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 ms, sys: 3.19 ms, total: 32.2 ms\n",
      "Wall time: 30.5 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>66</td>\n",
       "      <td>0.291851</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>100</td>\n",
       "      <td>0.251752</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>72</td>\n",
       "      <td>0.274083</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>77</td>\n",
       "      <td>0.277688</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>50</td>\n",
       "      <td>0.298834</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>83</td>\n",
       "      <td>0.259953</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>55</td>\n",
       "      <td>0.288206</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>88</td>\n",
       "      <td>0.248065</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>61</td>\n",
       "      <td>0.272023</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Netflix_2014</td>\n",
       "      <td>94</td>\n",
       "      <td>0.252153</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>66</td>\n",
       "      <td>0.221312</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>100</td>\n",
       "      <td>0.208859</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>72</td>\n",
       "      <td>0.189652</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>77</td>\n",
       "      <td>0.228360</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix_2008</td>\n",
       "      <td>50</td>\n",
       "      <td>0.244658</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        company  topics  coherence  \\\n",
       "0  Netflix_2014      66   0.291851   \n",
       "1  Netflix_2014     100   0.251752   \n",
       "2  Netflix_2014      72   0.274083   \n",
       "3  Netflix_2014      77   0.277688   \n",
       "4  Netflix_2014      50   0.298834   \n",
       "5  Netflix_2014      83   0.259953   \n",
       "6  Netflix_2014      55   0.288206   \n",
       "7  Netflix_2014      88   0.248065   \n",
       "8  Netflix_2014      61   0.272023   \n",
       "9  Netflix_2014      94   0.252153   \n",
       "0  Netflix_2008      66   0.221312   \n",
       "1  Netflix_2008     100   0.208859   \n",
       "2  Netflix_2008      72   0.189652   \n",
       "3  Netflix_2008      77   0.228360   \n",
       "4  Netflix_2008      50   0.244658   \n",
       "\n",
       "                                              models  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  \n",
       "6  LatentDirichletAllocation(learning_method='onl...  \n",
       "7  LatentDirichletAllocation(learning_method='onl...  \n",
       "8  LatentDirichletAllocation(learning_method='onl...  \n",
       "9  LatentDirichletAllocation(learning_method='onl...  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_list = []\n",
    "for data in output_dictionary2.keys():\n",
    "    temp_df = pd.DataFrame.from_dict(output_dictionary2[data])\n",
    "    dfs_list.append(temp_df)\n",
    "\n",
    "output_dfs = pd.concat(dfs_list)\n",
    "output_dfs.columns = ['company', 'topics', 'coherence', 'models']\n",
    "output_dfs.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best topic based on the new output, and get the top 10 words per topic. At the moment, you are only adding 1 of the topics for each company but you can change this by removing the indexing in `top_topics` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 ms, sys: 7.65 ms, total: 60.4 ms\n",
      "Wall time: 60 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_topics_model = defaultdict(tuple) # the output goes here\n",
    "\n",
    "for company, vrizer in zip(unique_ids, vectorizers_list):\n",
    "    cond = output_dfs['company'] == company # get each company\n",
    "    filtered_data = output_dfs[cond] # to get a single dataframe\n",
    "    the_topic = int(filtered_data.loc[filtered_data['coherence'].idxmax(), 'topics']) # get the best topic based on max coherence\n",
    "    the_model = filtered_data.loc[filtered_data['coherence'].idxmax(), 'models'] # get the best model based on max coherence\n",
    "    top_topics = show_topics(vrizer[2], the_model, 10) # get the top 10 words for each topic in each company\n",
    "    best_topics_model[company] = (the_topic, the_model, top_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(tuple,\n",
       "            {'Netflix_2014': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['posit', 'polit', 'fellow', 'remark', 'sever', 'reason', 'could',\n",
       "                     'product', 'improv', 'stretch'], dtype='<U13')),\n",
       "             'Netflix_2008': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['work', 'cultur', 'ego', 'peopl', 'number', 'quickli', 'passion',\n",
       "                     'talent', 'address', 'respons'], dtype='<U14')),\n",
       "             'Netflix_2009': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['good', 'custom', 'standard', 'delight', 'ive', 'right', 'wors',\n",
       "                     'new', 'felt', 'fresh'], dtype='<U16')),\n",
       "             'Netflix_2010': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['nice', 'benefit', 'employe', 'time', 'custom', 'work', 'stock',\n",
       "                     'perk', 'entic', 'ive'], dtype='<U16')),\n",
       "             'Netflix_2011': (18,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=18, random_state=1234),\n",
       "              array(['expect', 'execut', 'gato', 'sync', 'averagecatch', 'lo', 'take',\n",
       "                     'call', '22', 'qualif'], dtype='<U15')),\n",
       "             'Netflix_2012': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['worker', 'without', 'ink', 'decid', 'open', 'appli', 'assum',\n",
       "                     'life', 'center', 'densiti'], dtype='<U13')),\n",
       "             'Netflix_2013': (23,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=23, random_state=1234),\n",
       "              array(['peopl', 'develop', 'emphas', 'tri', 'light', 'excel', 'keep',\n",
       "                     'manag', 'high', 'ever'], dtype='<U11')),\n",
       "             'Netflix_2015': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['coach', 'work', 'help', 'pay', 'behavior', 'enough', 'drama',\n",
       "                     'environ', 'fair', 'lucki'], dtype='<U23')),\n",
       "             'Netflix_2016': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['great', 'work', 'cultur', 'respons', 'request', 'lot', 'opportun',\n",
       "                     'neck', 'plan', 'compani'], dtype='<U38')),\n",
       "             'Netflix_2017': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['free', 'good', 'clock', 'market', 'thought', 'code', 'dental',\n",
       "                     'lot', 'moder', 'coffe'], dtype='<U16')),\n",
       "             'Netflix_2018': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['environ', 'start', 'schedul', 'back', 'allow', '24', 'option',\n",
       "                     'perfect', 'feel', 'pretti'], dtype='<U11'))})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_topics_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the probabilities dataframes for each company and add them to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "for tup in vectorizers_list:\n",
    "    docs_of_probas[tup[0]] = pd.DataFrame(best_topics_model[tup[0]][1].transform(tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it with any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_of_probas['Hays'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the measures of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 s, sys: 9.17 ms, total: 4.95 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comP_h_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comP_h_results[company] = comph(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Netflix_2014': 0.6159537428190394,\n",
       "             'Netflix_2008': 0.6234709516953487,\n",
       "             'Netflix_2009': 0.6179573645221192,\n",
       "             'Netflix_2010': 0.6113292241115493,\n",
       "             'Netflix_2011': 0.5765823150356264,\n",
       "             'Netflix_2012': 0.5986994562659044,\n",
       "             'Netflix_2013': 0.5915132386324783,\n",
       "             'Netflix_2015': 0.5813203068464268,\n",
       "             'Netflix_2016': 0.604695711450533,\n",
       "             'Netflix_2017': 0.5739279502998358,\n",
       "             'Netflix_2018': 0.577656344202352})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comP_h_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 0 ns, total: 17.2 ms\n",
      "Wall time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "comT_h_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comT_h_results[company] = conth(proba_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Netflix_2014': 1.1888427351671123,\n",
       "             'Netflix_2008': 1.145146248507382,\n",
       "             'Netflix_2009': 1.1620342844458302,\n",
       "             'Netflix_2010': 1.1736365931630586,\n",
       "             'Netflix_2011': 1.1478207553814113,\n",
       "             'Netflix_2012': 1.192052936611861,\n",
       "             'Netflix_2013': 1.1880075318092609,\n",
       "             'Netflix_2015': 1.2756316247431176,\n",
       "             'Netflix_2016': 1.24874917605445,\n",
       "             'Netflix_2017': 1.3057914470387517,\n",
       "             'Netflix_2018': 1.2872432056066183})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comT_h_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_avg(probMatrix):\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(sp.stats.entropy(probMatrix[i]))\n",
    "    return np.mean(entropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 247 Âµs, total: 27.1 ms\n",
      "Wall time: 26.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "entropy_avg_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    entropy_avg_results[company] = ent_avg(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Netflix_2014': 0.5913163664254844,\n",
       "             'Netflix_2008': 0.4724837369413857,\n",
       "             'Netflix_2009': 0.5252152141081612,\n",
       "             'Netflix_2010': 0.5136035961652106,\n",
       "             'Netflix_2011': 0.42430919282915924,\n",
       "             'Netflix_2012': 0.5703767278887092,\n",
       "             'Netflix_2013': 0.5326530746454763,\n",
       "             'Netflix_2015': 0.7587162691431096,\n",
       "             'Netflix_2016': 0.713202232190446,\n",
       "             'Netflix_2017': 0.7769405041498997,\n",
       "             'Netflix_2018': 0.7687024017462769})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross-entropy of two probability distributions\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i] * np.log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "# function to compute the average cross-entropy of a matrix\n",
    "def avg_crossEnt(probMatrix): \n",
    "#    NOTE: Cross entropy is not symmetric. \n",
    "#    This function takes both cross-entropy(p,q) and cross-entropy(q,p) \n",
    "#    into account when computing the avg\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i != j:\n",
    "                crossEntropy_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "    return np.mean(crossEntropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.37 s, sys: 0 ns, total: 8.37 s\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_entropy_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    cross_entropy_results[company] = avg_crossEnt(proba_df.values)\n",
    "    \n",
    "# avg_crossEnt(docs_topics_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Netflix_2014': 9.282826670317775,\n",
       "             'Netflix_2008': 9.75645409295032,\n",
       "             'Netflix_2009': 9.47084535777085,\n",
       "             'Netflix_2010': 8.677540106866953,\n",
       "             'Netflix_2011': 7.655200136887355,\n",
       "             'Netflix_2012': 8.436132466440842,\n",
       "             'Netflix_2013': 7.948402588834676,\n",
       "             'Netflix_2015': 8.71140154771855,\n",
       "             'Netflix_2016': 9.071257343746264,\n",
       "             'Netflix_2017': 8.035673195990254,\n",
       "             'Netflix_2018': 7.935449449271206})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
