{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from joblib import parallel_backend\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "import math, re\n",
    "import numpy as np\n",
    "import csv\n",
    "import concurrent.futures as cf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "# from numba import jit\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693, 39)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix_df = pd.read_csv(\"netflix.csv\")\n",
    "netflix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize_snow(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "        \n",
    "def tokenize_lemma(text):\n",
    "    stem = nltk.wordnet.WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "        \n",
    "def tokenize_stem(text):\n",
    "    stem = nltk.stem.PorterStemmer()\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(nopython=True)\n",
    "def jsd(p, q, base=np.e): # JS distance between probability vectors, used to compute compH\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = 1 / 2 * (p + q)\n",
    "    \n",
    "    return sp.stats.entropy(p, m, base=base) / 2 +  sp.stats.entropy(q, m, base=base) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(nopython=True)\n",
    "def conth(p_mtx_df): # function to measure content heterogeneity given a topic (prob) matrix\n",
    "    '''\n",
    "    How is this review spread across the topics.\n",
    "    Then you take the average values across the reviews\n",
    "    Herfindall index\n",
    "    Assuming the reviews are about culture\n",
    "    Are people on average focus on a few cultural values (topics) when they write their review\n",
    "    '''\n",
    "    return (1 / ((sum(map(sum, np.square(p_mtx_df.values)))) / p_mtx_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to have an additional argument. Or be completely redisigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comph(probMatrix_df, arr_or_df='df'): \n",
    "\n",
    "    if arr_or_df == 'df':\n",
    "        probMatrix = probMatrix_df\n",
    "    else:\n",
    "        probMatrix = probMatrix_df.values\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for x in range(len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for y in range(len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[x], probMatrix[y]))\n",
    "        df[str(x)] = jsd_list\n",
    "\n",
    "    #Get df lower diagonal\n",
    "    mask = np.ones(df.shape, dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1) & mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    for k in df.columns: \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        col_array = df_lower_diagonal.loc[df_lower_diagonal[k].notna(), k].values\n",
    "        for d in col_array:\n",
    "            distance_list.append(d)\n",
    "\n",
    "    return (sum(distance_list) / len(distance_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_name_out(data, col_to_search, col_reviews, companies_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, the name of the column with all of \n",
    "    the companies, the name of the column with the reviews, and an iterable\n",
    "    with the companies names that are in the dataset. The latter could be a list,\n",
    "    set, Series, tuple, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    for company in companies_list:\n",
    "        condition = (data[col_to_search] == company)\n",
    "        data.loc[condition, col_reviews] = data.loc[condition, col_reviews].str.lower().str.strip(company.lower())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_list = ['Netflix', 'amazon'] \n",
    "netflix_df = comp_name_out(netflix_df, 'employerName', 'pros', comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you will be working with the most talented ppl around.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pros = netflix_df['pros'].values\n",
    "data_pros[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens]\n",
    "#     filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "corp_normalizer = np.vectorize(normalize_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.52046783625731"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with cf.ProcessPoolExecutor() as executor:\n",
    "    data_pros_cleaned = executor.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(data_pros_cleaned)\n",
    "\n",
    "TotalWords_vectorizer = CountVectorizer(stop_words='english')\n",
    "TotalWords_tf = TotalWords_vectorizer.fit_transform(data_pros_cleaned)\n",
    "totWords = len(TotalWords_vectorizer.get_feature_names())\n",
    "# totWords\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(tokenizer=tokenize_snow, max_df = 0.90, min_df=0.01)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_pros_cleaned)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "percVoc = len(tf_feature_names) / totWords * 100\n",
    "percVoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with topic 2\n",
      "Done with topic 7\n",
      "Done with topic 12\n",
      "Done with topic 17\n",
      "Done with topic 22\n",
      "Done with topic 27\n",
      "Done with topic 32\n",
      "Done with topic 37\n",
      "Done with topic 42\n",
      "Done with topic 47\n",
      "Done with topic 52\n",
      "Done with topic 57\n",
      "Done with topic 62\n",
      "Done with topic 67\n",
      "Done with topic 72\n",
      "Done with topic 77\n",
      "Done with topic 82\n",
      "Done with topic 87\n",
      "Done with topic 92\n",
      "Done with topic 97\n",
      "Done with topic 102\n",
      "Done with topic 107\n",
      "Done with topic 112\n",
      "Done with topic 117\n",
      "Done with topic 122\n",
      "Done with topic 127\n",
      "Done with topic 132\n",
      "Done with topic 137\n",
      "Done with topic 142\n",
      "Done with topic 147\n",
      "Done with topic 152\n",
      "Done with topic 157\n",
      "Done with topic 162\n",
      "Done with topic 167\n",
      "Done with topic 172\n",
      "Done with topic 177\n",
      "Done with topic 182\n",
      "Done with topic 187\n",
      "Done with topic 192\n",
      "Done with topic 197\n",
      "Done with topic 202\n",
      "Done with topic 207\n",
      "Done with topic 212\n",
      "Done with topic 217\n",
      "Done with topic 222\n",
      "Done with topic 227\n",
      "Done with topic 232\n",
      "Done with topic 237\n",
      "Done with topic 242\n",
      "Done with topic 247\n",
      "Done with topic 252\n",
      "Done with topic 257\n",
      "Done with topic 262\n",
      "Done with topic 267\n",
      "Done with topic 272\n",
      "Done with topic 277\n",
      "Done with topic 282\n",
      "Done with topic 287\n",
      "Done with topic 292\n",
      "Done with topic 297\n",
      "CPU times: user 37min 46s, sys: 15min 16s, total: 53min 3s\n",
      "Wall time: 1h 5min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "i = 0    \n",
    "output=np.zeros((60,3))\n",
    "\n",
    "#totWordsPerdocument = np.sum(tf_matrix, axis=1)\n",
    "for topics in range(2,300,5): \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=topics, max_iter=200, \n",
    "                                    learning_method='online',random_state=1234, n_jobs=-1)\n",
    "    lda_fit = lda.fit(tf)\n",
    "    #output normalized matrix with distributions of topics over words\n",
    "    #normalized\n",
    "    topicsOverWords = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "    topicsDissim_avg = comph(topicsOverWords)\n",
    "\n",
    "#store results per firm   \n",
    "    output[i,0] = topics\n",
    "    output[i,1] = topicsDissim_avg \n",
    "    output[i,2] = percVoc\n",
    "  \n",
    "    i = i+1\n",
    "    \n",
    "    print(f'Done with topic {topics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "output = defaultdict(np.float32)\n",
    "\n",
    "for i, topics in enumerate(range(2, 300, 5)):\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=topics, max_iter=200, learning_method='online', \n",
    "                                    random_state=1234, n_jobs=-1)\n",
    "    ldamo = lda.fit(tf)\n",
    "\n",
    "    #output normalized matrix with distributions of topics over words\n",
    "    #normalized\n",
    "    topicsOverWords = ldamo.components_ / ldamo.components_.sum(axis=1)[:, np.newaxis]\n",
    "    topicsDissim_avg = comph(topicsOverWords)\n",
    "\n",
    "    #store results per firm   \n",
    "    output[i] = (topics, topicsDissim_avg, percVoc)\n",
    "    print(f'Done with topic {topics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# to use joblib with dask we need a client\n",
    "# client = Client(processes=False)\n",
    "mo_list = []\n",
    "\n",
    "\n",
    "for topics in range(2, 300, 5):\n",
    "    with joblib.parallel_backend('dask'):\n",
    "        lda = LatentDirichletAllocation(n_components=topics, max_iter=200, learning_method='batch', \n",
    "                                        learning_offset=10., evaluate_every=2, random_state=1234)#, n_jobs=-1)\n",
    "        ldamo = lda.fit(tf)\n",
    "        mo_list.append((topics, ldamo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = (ldamo.components_ / ldamo.components_.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>voc%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.299548</td>\n",
       "      <td>16.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.390344</td>\n",
       "      <td>16.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.432444</td>\n",
       "      <td>16.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.418613</td>\n",
       "      <td>16.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.424082</td>\n",
       "      <td>16.520468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topics  coherence       voc%\n",
       "0     2.0   0.299548  16.520468\n",
       "1     7.0   0.390344  16.520468\n",
       "2    12.0   0.432444  16.520468\n",
       "3    17.0   0.418613  16.520468\n",
       "4    22.0   0.424082  16.520468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_coherence_df = pd.DataFrame(output, columns=['topics', 'coherence', 'voc%'])\n",
    "topics_coherence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_topics = int(topics_coherence_df.loc[topics_coherence_df['coherence'].idxmax(), 'topics'])\n",
    "optimal_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=optimal_topics,\n",
    "                                max_iter=200, \n",
    "                                learning_method='batch', \n",
    "                                learning_offset=10.,\n",
    "                                evaluate_every=2,\n",
    "                                random_state=1234,\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "probMatrix = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.33362002e-03, 8.33358403e-03, 8.33344468e-03, ...,\n",
       "        8.33378128e-03, 4.36523807e-01, 8.33370994e-03],\n",
       "       [4.10284195e-02, 3.68742235e-04, 3.68753442e-04, ...,\n",
       "        2.51008316e-01, 7.04644549e-01, 3.68751822e-04],\n",
       "       [1.66668211e-02, 1.66667274e-02, 8.16664791e-01, ...,\n",
       "        1.66668440e-02, 1.66667517e-02, 1.66668555e-02],\n",
       "       ...,\n",
       "       [2.77782128e-02, 2.77792629e-02, 2.77779631e-02, ...,\n",
       "        2.77780380e-02, 2.77779938e-02, 2.77782421e-02],\n",
       "       [4.62984546e-03, 4.57358786e-01, 4.62973700e-03, ...,\n",
       "        4.62983456e-03, 4.62990910e-03, 4.62979304e-03],\n",
       "       [9.25970965e-03, 9.25946955e-03, 8.98145585e-01, ...,\n",
       "        9.25931826e-03, 9.25932319e-03, 9.25934628e-03]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.236404</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.008334</td>\n",
       "      <td>0.436524</td>\n",
       "      <td>0.008334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041028</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.251008</td>\n",
       "      <td>0.704645</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.816665</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.058097</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.054277</td>\n",
       "      <td>0.074014</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.442668</td>\n",
       "      <td>0.169775</td>\n",
       "      <td>0.196407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.093446</td>\n",
       "      <td>0.171070</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.079256</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.336622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.008334  0.008334  0.008333  0.236404  0.252069  0.008334  0.008333   \n",
       "1  0.041028  0.000369  0.000369  0.000369  0.000369  0.000369  0.000369   \n",
       "2  0.016667  0.016667  0.816665  0.016667  0.016667  0.016667  0.016667   \n",
       "3  0.000794  0.058097  0.000794  0.000794  0.000794  0.000794  0.054277   \n",
       "4  0.001302  0.093446  0.171070  0.001302  0.001302  0.079256  0.001302   \n",
       "\n",
       "         7         8         9         10        11  \n",
       "0  0.008334  0.008334  0.008334  0.436524  0.008334  \n",
       "1  0.000369  0.000369  0.251008  0.704645  0.000369  \n",
       "2  0.016667  0.016667  0.016667  0.016667  0.016667  \n",
       "3  0.074014  0.000794  0.442668  0.169775  0.196407  \n",
       "4  0.001302  0.001302  0.310491  0.001302  0.336622  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_topics_df = pd.DataFrame(data = probMatrix)\n",
    "docs_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 96.8 ms, total: 25.6 s\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4907799214962477"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comp_H = comph(docs_topics_df, arr_or_df='arr')\n",
    "comp_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 ms, sys: 48 µs, total: 2.66 ms\n",
      "Wall time: 2.63 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7702037303029805"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "conT_H = conth(docs_topics_df)\n",
    "conT_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d4e7161a4a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ment_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_topics_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d4e7161a4a76>\u001b[0m in \u001b[0;36ment_avg\u001b[0;34m(probMatrix)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mentropy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mentropy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobMatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mentropy_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentropy_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entropy' is not defined"
     ]
    }
   ],
   "source": [
    "def ent_avg(probMatrix): \n",
    "    import statistics\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(entropy(probMatrix[i]))\n",
    "    entropy_avg = statistics.mean(entropy_list)\n",
    "    return entropy_avg    \n",
    "\n",
    "ent_avg(docs_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross-entropy of two probability distributions\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i]*log2(q[i]) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the average cross-entropy of a matrix\n",
    "def avg_crossEnt(probMatrix): \n",
    "#    NOTE: Cross entropy is not symmetric. \n",
    "#    This function takes both cross-entropy(p,q) and cross-entropy(q,p) \n",
    "#    into account when computing the avg\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i != j:\n",
    "                crossEntropy_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "    crossEntropy_avg = statistics.mean(crossEntropy_list)\n",
    "    return crossEntropy_avg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
