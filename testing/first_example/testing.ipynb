{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "import scipy as sp\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = pd.read_pickle(\"Netflix_Data\")\n",
    "netflix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pros = netflix_df['pros'].values\n",
    "data_pros[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_of_word(docs, root_word_method='lemma'):\n",
    "    \n",
    "    porter_stemmer = nltk.stem.PorterStemmer()\n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(docs)\n",
    "    \n",
    "    if root_word_method == 'lemma':\n",
    "        doc = ' '.join([lemma.lemmatize(w) for w in tokens])\n",
    "    elif root_word_method == 'stemm':\n",
    "        doc = ' '.join([porter_stemmer.stem(w) for w in tokens])\n",
    "    elif root_word_method == 'snowball':\n",
    "        doc = ' '.join([snowball_stemmer.stem(w) for w in tokens])\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = partial(root_of_word, root_word_method='stemm')\n",
    "snowball = partial(root_of_word, root_word_method='snowball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(p, q, base=np.e): # JS distance between probability vectors, used to compute compH\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = 1./2*(p + q)\n",
    "    return sp.stats.entropy(p, m, base=base) / 2. +  sp.stats.entropy(q, m, base=base) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conth(prob_matrix_df): # function to measure content heterogeneity given a topic (prob) matrix\n",
    "    N = prob_matrix_df.shape[0]\n",
    "    probMatrix = prob_matrix_df.values\n",
    "    conth = 1/((sum(map(sum, np.square(probMatrix))))/N)\n",
    "    return conth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to have an additional argument. Or be completely redisigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comph(probMatrix_df, arr_or_df='df'): \n",
    "    #Transform probMatrix_df into 2D array\n",
    "    \n",
    "    if arr_or_df == 'df':\n",
    "        probMatrix = probMatrix_df\n",
    "    else:\n",
    "        probMatrix = probMatrix_df.values\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for x in range(0, len(probMatrix)): \n",
    "        jsd_list = []\n",
    "        for y in range(0, len(probMatrix)): \n",
    "            jsd_list.append(jsd(probMatrix[x], probMatrix[y]))\n",
    "            y = y+1\n",
    "        df[str(x)] = jsd_list\n",
    "\n",
    "\n",
    "    #Get df lower diagonal\n",
    "    mask = np.ones(df.shape, dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1) & mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    k = 0 \n",
    "    for k in range(0, len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(k)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [l for l in column_list if (math.isnan(l) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "        k = k + 1\n",
    "    comph = sum(distance_list) / float(len(distance_list))\n",
    "    return comph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pros_clean(item):\n",
    "    if item != \"\":\n",
    "        item = item.lower().replace(\"netflix\", \" \")\n",
    "        item = item.replace(\"show less\", \"\")\n",
    "        item = item.replace(\"show more\", \"\")\n",
    "        item = item.replace(\"\\n\", \"\")\n",
    "        item_modified =  ''.join([i for i in item if not i.isdigit()])\n",
    "    return item_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cf.ProcessPoolExecutor() as executor:\n",
    "    data_pros_cleaned = executor.map(get_pros_clean, data_pros)\n",
    "    data_pros_cleaned = list(data_pros_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TotalWords_vectorizer = CountVectorizer(stop_words='english')\n",
    "TotalWords_tf = TotalWords_vectorizer.fit_transform(data_pros_cleaned)\n",
    "totWords = len(TotalWords_vectorizer.get_feature_names())\n",
    "totWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cf.ProcessPoolExecutor() as executor:\n",
    "    snowball_pros_cleaned = executor.map(snowball, data_pros_cleaned)\n",
    "    snowball_pros_cleaned = list(snowball_pros_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df = 0.90, min_df=0.01, stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(snowball_pros_cleaned)\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.603531300160512"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percVoc = len(tf_feature_names) / totWords * 100\n",
    "percVoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 28.3 s, total: 4min 43s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "i = 0    \n",
    "output=np.zeros((60,3))\n",
    "\n",
    "for topics in range(2, 300, 5): \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=topics, \n",
    "                                    max_iter=200, \n",
    "                                    evaluate_every=2,\n",
    "                                    random_state=1234,\n",
    "                                    n_jobs=-1).\n",
    "    lda_fit = lda.fit(tf)\n",
    "    #output normalized matrix with distributions of topics over words\n",
    "    #normalized\n",
    "    topicsOverWords = lda_fit.components_ / lda_fit.components_.sum(axis=1)[:, np.newaxis]\n",
    "    topicsDissim_avg = comph(topicsOverWords)\n",
    "\n",
    "    #store results per firm   \n",
    "    output[i,0] = topics\n",
    "    output[i,1] = topicsDissim_avg \n",
    "    output[i,2] = percVoc\n",
    "  \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>voc%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.332338</td>\n",
       "      <td>17.955508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.378940</td>\n",
       "      <td>17.955508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.399296</td>\n",
       "      <td>17.955508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.420066</td>\n",
       "      <td>17.955508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.428053</td>\n",
       "      <td>17.955508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topics  coherence       voc%\n",
       "0     2.0   0.332338  17.955508\n",
       "1     7.0   0.378940  17.955508\n",
       "2    12.0   0.399296  17.955508\n",
       "3    17.0   0.420066  17.955508\n",
       "4    22.0   0.428053  17.955508"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_save = (\"TopicInterpretation_Netflix_Pros_OptimalTopics_Coherence.csv\")\n",
    "\n",
    "out_df = pd.DataFrame(output, columns=['topics', 'coherence', 'voc%'])\n",
    "out_df.to_csv(filename_save, index=False)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_coherence_df = pd.read_csv(filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_topics = int(topics_coherence_df.loc[topics_coherence_df['coherence'].idxmax(), 'topics'])\n",
    "optimal_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=int(optimal_topics),\n",
    "                                max_iter=200, \n",
    "                                learning_method='batch', \n",
    "                                learning_offset=10.,\n",
    "                                evaluate_every=2,\n",
    "                                random_state=1234,\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "probMatrix = lda.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.73611111e-03, 1.73611111e-03, 1.73611111e-03, ...,\n",
       "        1.73611111e-03, 1.73611111e-03, 1.73611111e-03],\n",
       "       [6.20039683e-05, 6.20039683e-05, 6.20039683e-05, ...,\n",
       "        6.20039683e-05, 6.20039683e-05, 6.20039683e-05],\n",
       "       [1.04166667e-03, 1.04166667e-03, 1.04166667e-03, ...,\n",
       "        1.04166667e-03, 1.04166667e-03, 1.04166667e-03],\n",
       "       ...,\n",
       "       [1.30208333e-03, 1.30208333e-03, 1.30208333e-03, ...,\n",
       "        1.30208333e-03, 1.30208333e-03, 1.30208333e-03],\n",
       "       [4.73484848e-04, 4.73484848e-04, 4.73484848e-04, ...,\n",
       "        4.73484848e-04, 4.73484848e-04, 4.73484848e-04],\n",
       "       [5.78703704e-04, 5.78703704e-04, 5.78703704e-04, ...,\n",
       "        2.14779009e-01, 5.78703704e-04, 5.78703704e-04]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_topics_df = pd.DataFrame(data = probMatrix)\n",
    "docs_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5969588015298183"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comp_H = comph(docs_topics_df, arr_or_df='arr')\n",
    "comp_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.7 ms, sys: 477 Âµs, total: 24.1 ms\n",
      "Wall time: 24.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.269603512937508"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "conT_H = conth(docs_topics_df)\n",
    "conT_H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
