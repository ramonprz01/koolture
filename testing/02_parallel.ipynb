{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk, re\n",
    "# nltk.download('wordnet')\n",
    "import scipy as sp\n",
    "import math\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import csv\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('nameoffile', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = pd.read_pickle(\"Netflix_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will remove the company names from their respective reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def comp_name_out(data, col_to_search, col_reviews, companies_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, the name of the column with all of \n",
    "    the companies, the name of the column with the reviews, and an iterable\n",
    "    with the companies names that are in the dataset. The latter could be a list,\n",
    "    set, Series, tuple, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    for company in companies_list:\n",
    "        condition = (data[col_to_search] == company)\n",
    "        data.loc[condition, col_reviews] = data.loc[condition, col_reviews].str.lower().str.strip(company.lower())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps with the preprocessing of the data. It runs after the lemmatizer, stemmer, snowball, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "#     filtered_tokens = [token for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the root of the word. You can get all three (lemma, stem, and snow) or use them separately with the partial functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_of_word(docs, root_word_method='lemma'):\n",
    "    porter_stemmer = nltk.stem.PorterStemmer()\n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(docs)\n",
    "    \n",
    "    if root_word_method == 'lemma':\n",
    "        doc = ' '.join([lemma.lemmatize(w) for w in tokens])\n",
    "    elif root_word_method == 'stemm':\n",
    "        doc = ' '.join([porter_stemmer.stem(w) for w in tokens])\n",
    "    elif root_word_method == 'snowball':\n",
    "        doc = ' '.join([snowball_stemmer.stem(w) for w in tokens])\n",
    "        \n",
    "    return doc\n",
    "\n",
    "stemming = partial(root_of_word, root_word_method='stemm')\n",
    "snowball = partial(root_of_word, root_word_method='snowball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(p, q, base=np.e): # JS distance between probability vectors, used to compute compH\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = (1 / 2 * (p + q))\n",
    "    return sp.stats.entropy(p, m, base) / 2 +  sp.stats.entropy(q, m, base) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conth(data): # function to measure content heterogeneity given a topic (prob) matrix\n",
    "    return (1 / ((sum(map(sum, np.square(data.values)))) / data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comph(data): \n",
    "    #Transform probMatrix_df into 2D array\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    for x in range(len(data)): \n",
    "        jsd_list = []\n",
    "        for y in range(len(data)): \n",
    "            jsd_list.append(jsd(data[x], data[y]))\n",
    "        df[str(x)] = jsd_list\n",
    "\n",
    "    #Get df lower diagonal\n",
    "    mask = np.ones(df.shape, dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1) & mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    for k in range(len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(k)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [l for l in column_list if (math.isnan(l) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "            \n",
    "    return sum(distance_list) / float(len(distance_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the company names from the reviews, and extract the reviews into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = ['Netflix', 'amazon'] \n",
    "netflix_df = comp_name_out(netflix_df, 'employerName', 'pros', comp_list)\n",
    "data_pros = netflix_df['pros'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 947 ms, sys: 16.9 ms, total: 964 ms\n",
      "Wall time: 1.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['work talent ppl around',\n",
       "       'reedom respons treat like adult part pro team highli function compani well respect ha super posit brand awar never abl go anywher without get pepper rave happi custom comment love wear compani logo gear thi reason matter netflix dead wood everyon someth veri import compani would nt matter make differ feel good realli make differ matter role compani often repeat employe olymp team mean nt thi 247 life focu gold varsiti team play win veri good still balanc class learn need good worklif balanc veri import compani think best thing subtl thi vacationno holiday stuff work work seem realiz everyon work hard time night weekend often ton hard work want take time vacat long holiday day whatever happen discret nt ask nt get permiss one keep track never heard thi kind polici imagin doest moral feel like treat like grown rule alon think stand beyond ani org',\n",
       "       'great colleagu incred realli'], dtype='<U1247')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(stemming, data_pros)\n",
    "    data_pros_cleaned = list(e.map(normalize_doc, data_pros_cleaned))\n",
    "data_pros_cleaned[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total words in the dictionary of review words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TotalWords_vectorizer = CountVectorizer()\n",
    "TotalWords_tf = TotalWords_vectorizer.fit_transform(data_pros)\n",
    "totWords = len(TotalWords_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the cleaned dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df = 0.90, min_df=0.01)\n",
    "tf = tf_vectorizer.fit_transform(data_pros_cleaned)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of words in the final dictionary that can be found in the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.981220657276996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percVoc = len(tf_feature_names) / totWords * 100\n",
    "percVoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to parallelize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_range = range(2, 300, 5)\n",
    "\n",
    "def get_models(topics):\n",
    "    lda = LatentDirichletAllocation(n_components=topics, max_iter=200, \n",
    "                                    learning_method='batch', learning_offset=10.,\n",
    "                                    evaluate_every=2, random_state=1234)\n",
    "    \n",
    "    lda_model = lda.fit(tf)\n",
    "    topicsOverWords = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    return (topics, comph(topicsOverWords), lda_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 ms, sys: 181 ms, total: 335 ms\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output = list(e.map(get_models, our_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.309376</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.359481</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0.418974</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>0.447498</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>0.444542</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topics  coherence                                             models\n",
       "0       2   0.309376  LatentDirichletAllocation(evaluate_every=2, ma...\n",
       "1       7   0.359481  LatentDirichletAllocation(evaluate_every=2, ma...\n",
       "2      12   0.418974  LatentDirichletAllocation(evaluate_every=2, ma...\n",
       "3      17   0.447498  LatentDirichletAllocation(evaluate_every=2, ma...\n",
       "4      22   0.444542  LatentDirichletAllocation(evaluate_every=2, ma..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = pd.DataFrame(output, columns=['topics', 'coherence', 'models'])\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_topics = int(out_df.loc[out_df['coherence'].idxmax(), 'topics'])\n",
    "optimal_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>172</td>\n",
       "      <td>0.584355</td>\n",
       "      <td>LatentDirichletAllocation(evaluate_every=2, ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topics  coherence                                             models\n",
       "34     172   0.584355  LatentDirichletAllocation(evaluate_every=2, ma..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df[out_df['topics'] == optimal_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = out_df.loc[out_df['topics'] == optimal_topics, 'models'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_topics_df = pd.DataFrame(best_model.transform(tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the measures of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 s, sys: 138 ms, total: 25.6 s\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6001764879390206"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comP_H = comph(docs_topics_df.values)\n",
    "comP_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 1.37 ms, total: 21.8 ms\n",
      "Wall time: 22.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.068080930909674"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "conT_H = conth(docs_topics_df)\n",
    "conT_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_avg(probMatrix):\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(sp.stats.entropy(probMatrix[i]))\n",
    "    entropy_avg = np.mean(entropy_list)\n",
    "    return entropy_avg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5340804507420907"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_avg(docs_topics_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross-entropy of two probability distributions\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i] * np.log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "# function to compute the average cross-entropy of a matrix\n",
    "def avg_crossEnt(probMatrix): \n",
    "#    NOTE: Cross entropy is not symmetric. \n",
    "#    This function takes both cross-entropy(p,q) and cross-entropy(q,p) \n",
    "#    into account when computing the avg\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i != j:\n",
    "                crossEntropy_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "    return np.mean(crossEntropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.590204388743519"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "avg_crossEnt(docs_topics_df.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
