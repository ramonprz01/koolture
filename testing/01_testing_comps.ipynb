{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Culture Measures Based on Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "# nlkt.download('punkt')\n",
    "\n",
    "import koolture as kt\n",
    "\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean_gs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fist Range of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the company names from the reviews, and extract the reviews into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_of_interest = df.employer.value_counts()\n",
    "comps_of_interest.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comps_of_interest = (comps_of_interest).index\n",
    "comps_of_interest = (comps_of_interest[(comps_of_interest > 290)]).index\n",
    "len(comps_of_interest), comps_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = df['employer'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['employer'].unique() # get the unique IDs or unique employers in the dataset\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Custom Stopwords List Before Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel. You first normalize the reviews and then take the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_pros = df_interest['pros'].values\n",
    "stopwords = nltk.corpus.stopwords.words('english') + [token.lower() for token in unique_ids]\n",
    "stopwords[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_doc = partial(kt.normalize_doc, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(kt.root_of_word, data_pros_cleaned))\n",
    "\n",
    "df_interest['pros_clean'] = data_pros_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vectorizers Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers_dicts = kt.get_vectorizers(data=df_interest, unique_ids=unique_ids,\n",
    "                                      company_col='employer', reviews_col='pros', \n",
    "                                      vrizer=CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block run the models in parallel over the companies available and using the specifiedamount of topics in our_range variable and return a dictionary with the output of the get_models function for each company. It is used to identify the interval to search further for optimal topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "partial_func = partial(kt.get_models, topics=our_range, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output = list(e.map(partial_func, unique_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will now iterate over the dictionary output from above, add each dataset into a list, and then concatenate them all into one dataset (output df contains exactly same information, but more readable, and used in next blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = kt.build_dataframe(output)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates over the new dataframe, searches for the top 2 topics based on highest coherence, and appends to a list a tuple containing the company, a tuple with the top two topic numbers, and the fitted vectorizer from the original `vectorizers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topics_sorted, comps, tops = kt.top_two_topics(data=output_df, companies_var='company',\n",
    "                               coherence_var='coherence', topics_var='topics',\n",
    "                               unique_ids=unique_ids, vrizers_list=vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `get_models` function again over the new space of topics. You will  need to\n",
    "1. sort the tuple with the top two topics.\n",
    "2. create a linearly spaced array with 10 elements between the top 2 topics, turn it into integers, make the array a set to eliminate any duplicates that might arise if there is a 2 in the top two topics, and then turn that into a list.\n",
    "3. get your fixed partial function again\n",
    "4. the output is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "partial_func = partial(kt.get_models, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output2 = list(e.map(partial_func, comps, tops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple dataframes from dictionaries again and collapse them into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_df2 = kt.build_dataframe(output2)\n",
    "output_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best topic based on the new output, and get the top 10 words per topic. At the moment, you are only adding 1 of the topics for each company but you can change this by removing the indexing in `top_topics` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_topics = kt.absolute_topics(output_df2, 'company', 'coherence', \n",
    "                                 'topics', 'models', vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the probabilities dataframes for each company and add them to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "for tup in vectorizers_dicts.values():\n",
    "    docs_of_probas[tup[0]] = pd.DataFrame(best_topics[tup[0]][1].transform(tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the measures of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "comP_h_results = defaultdict(float)\n",
    "comT_h_results = defaultdict(float)\n",
    "entropy_avg_results = defaultdict(float)\n",
    "cross_entropy_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comP_h_results[company] = kt.comph(proba_df.values)\n",
    "    comT_h_results[company] = kt.conth(proba_df)\n",
    "    entropy_avg_results[company] = kt.ent_avg(proba_df.values)\n",
    "    cross_entropy_results[company] = kt.avg_crossEnt(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comph_df = pd.DataFrame.from_dict(comP_h_results.items())\n",
    "conth_df = pd.DataFrame.from_dict(comT_h_results.items())\n",
    "crossEnt_df = pd.DataFrame.from_dict(cross_entropy_results.items())\n",
    "cultureMetrics = comph_df.merge(conth_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics = cultureMetrics.merge(crossEnt_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics.columns = ['employerID', 'comph', 'conth', 'avgCrossEnt']\n",
    "cultureMetrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureMetrics.to_csv('CultureMetrics_TestSample_1000.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
