{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Culture Measures Based on Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employer</th>\n",
       "      <th>id</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Express</td>\n",
       "      <td>44001</td>\n",
       "      <td>Still not big enough in market place</td>\n",
       "      <td>Great brand , Good leadership , Clear business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Nothing important on my point of view.</td>\n",
       "      <td>Learn new technologies, helpful people, good m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Alot of friends working together which isn't v...</td>\n",
       "      <td>Very good opportunities to learn technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Working hours are not good and need to add the...</td>\n",
       "      <td>You can learn technically a lot in this company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>No Real Cons at all</td>\n",
       "      <td>- Very friendly environment.\\r\\n- Highly exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               employer     id  \\\n",
       "0      American Express  44001   \n",
       "1  Eventum IT Solutions  44004   \n",
       "2  Eventum IT Solutions  44004   \n",
       "3  Eventum IT Solutions  44004   \n",
       "4  Eventum IT Solutions  44004   \n",
       "\n",
       "                                                pros  \\\n",
       "0               Still not big enough in market place   \n",
       "1             Nothing important on my point of view.   \n",
       "2  Alot of friends working together which isn't v...   \n",
       "3  Working hours are not good and need to add the...   \n",
       "4                                No Real Cons at all   \n",
       "\n",
       "                                                cons  \n",
       "0  Great brand , Good leadership , Clear business...  \n",
       "1  Learn new technologies, helpful people, good m...  \n",
       "2      Very good opportunities to learn technologies  \n",
       "3   You can learn technically a lot in this company.  \n",
       "4  - Very friendly environment.\\r\\n- Highly exper...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_gs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will remove the company names from their respective reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def comp_name_out(data, col_to_search, col_reviews, companies):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, the name of the column with all of \n",
    "    the companies, the name of the column with the reviews, and an iterable\n",
    "    with the companies names that are in the dataset. The latter could be a list,\n",
    "    set, Series, tuple, etc.\n",
    "    \"\"\"\n",
    "    for company in companies:\n",
    "        condition = (data[col_to_search] == company)\n",
    "        data.loc[condition, col_reviews] = data.loc[condition, col_reviews].str.lower().str.replace(company.lower(), '', regex=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the root of the word. You can get all three (lemma, stem, and snow) or use them separately with the partial functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_of_word(docs, root_word_method='lemma'):\n",
    "    porter_stemmer = nltk.stem.PorterStemmer()\n",
    "    snowball_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(docs)\n",
    "    \n",
    "    if root_word_method == 'lemma':\n",
    "        doc = ' '.join([lemma.lemmatize(w) for w in tokens])\n",
    "    elif root_word_method == 'stemm':\n",
    "        doc = ' '.join([porter_stemmer.stem(w) for w in tokens])\n",
    "    elif root_word_method == 'snowball':\n",
    "        doc = ' '.join([snowball_stemmer.stem(w) for w in tokens])\n",
    "        \n",
    "    return doc\n",
    "\n",
    "stemming = partial(root_of_word, root_word_method='stemm')\n",
    "snowball = partial(root_of_word, root_word_method='snowball')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps with the preprocessing of the data. It runs after the lemmatizer, stemmer, snowball, etc. If you want to include stopwords and take them out at a later stage, uncomment the first `filtered_tokens` below and comment out the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words if you want to.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "#     filtered_tokens = [token for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to get the top topics and to run the LDA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words=15):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300\n",
    "\n",
    "def get_models(topics, tf, tup_num):\n",
    "    \"\"\"\n",
    "    This functions takes in the number of topics to run the model for,\n",
    "    a tuple with the name of the company and the sparse matix and\n",
    "    a number for the element in the tuple that has the sparse matix.\n",
    "    It then returns a tuple with (company name, topic #, comph, and the model)\n",
    "    \"\"\"\n",
    "    lda = LatentDirichletAllocation(n_components=topics, max_iter=100, learning_method='online', learning_offset=10., random_state=1234)\n",
    "    lda_model = lda.fit(tf[tup_num])\n",
    "    topicsOverWords = lda_model.components_ / lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    return (tf[0], topics, comph(topicsOverWords), lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsd(p, q, base=np.e): # JS distance between probability vectors, used to compute compH\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    m = (1 / 2 * (p + q))\n",
    "    return sp.stats.entropy(p, m, base) / 2 +  sp.stats.entropy(q, m, base) / 2\n",
    "\n",
    "def conth(data): # function to measure content heterogeneity given a topic (prob) matrix\n",
    "    return (1 / ((sum(map(sum, np.square(data.values)))) / data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comph(data): \n",
    "    #Transform probMatrix_df into 2D array\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    for x in range(len(data)): \n",
    "        jsd_list = []\n",
    "        for y in range(len(data)): \n",
    "            jsd_list.append(jsd(data[x], data[y]))\n",
    "        df[str(x)] = jsd_list\n",
    "\n",
    "    #Get df lower diagonal\n",
    "    mask = np.ones(df.shape, dtype='bool')\n",
    "    mask[np.triu_indices(len(df))] = False\n",
    "    df_lower_diagonal = df[(df>-1) & mask]\n",
    "    \n",
    "    distance_list = []\n",
    "    for k in range(len(df)): \n",
    "    #Transform each column of df_lower_diagonal into list\n",
    "        column_list = df_lower_diagonal[str(k)].values.tolist()\n",
    "        #Drop nan values from column_list - to retain only actual values from lower diagonal \n",
    "        column_lower_diagonal_list = [l for l in column_list if (math.isnan(l) == False)]\n",
    "        for d in column_lower_diagonal_list: \n",
    "            distance_list.append(d)\n",
    "            \n",
    "    return sum(distance_list) / float(len(distance_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array with the unique employers in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = df['employer'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the company names from the reviews, and extract the reviews into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 386 ms, total: 1min 34s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = comp_name_out(df, 'employer', 'pros', companies)\n",
    "data_pros = df['pros'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel. You first normalize the reviews and then take the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.7 s, sys: 9.99 s, total: 46.7 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(stemming, data_pros_cleaned))\n",
    "\n",
    "df['pros_clean'] = data_pros_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you create an array with all of the companies and the amount of reviews they have. So far, only companies with at least 2 reviews make it to the modeling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3207,\n",
       " Index(['Oney', 'Symantec', 'The NPD Group', 'Immedis', 'TMNA Services',\n",
       "        'Sprout Social', 'Isabel Group', 'Dachser', 'Noble Studios',\n",
       "        'LogiSense',\n",
       "        ...\n",
       "        'Jooble', 'EG', 'LeasePlan', 'Flying Tiger Copenhagen', 'Barhale',\n",
       "        'Rapyd', 'Klohn Crippen Berger', 'SwiftAnt IT Solutions', 'GRASS',\n",
       "        'Minami Studios'],\n",
       "       dtype='object', length=3207))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps_of_interest = df.employer.value_counts()\n",
    "comps_of_interest = (comps_of_interest[(comps_of_interest < 5) & (comps_of_interest > 1)]).index\n",
    "len(comps_of_interest), comps_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the employers that meet the condition above by creating a boolean with True for yes and False for no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = df['employer'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['employer'].unique() # get the unique IDs or unique employers in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will create sparse matrices for all companies and return a list of tuples with the name of the company, its sparse matrix, and the fitted vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.75 s, sys: 35.9 ms, total: 3.79 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers_list = []\n",
    "for comp_id in unique_ids:\n",
    "    cond = (df_interest['employer'] == comp_id) # condition to get a dataset for each company\n",
    "    revs_clean = df_interest.loc[cond, 'pros_clean'].values # get an array of reviews for such company\n",
    "    count_vect = CountVectorizer() # instantiate a vectorizer\n",
    "    vect = count_vect.fit_transform(revs_clean) # fit it to the selected company reviews\n",
    "    vectorizers_list.append((comp_id, vect, count_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total words in the dictionary of review words, and get the percentage of words in the final dictionary that can be found in the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalWords_vectorizer = CountVectorizer()\n",
    "# TotalWords_tf = TotalWords_vectorizer.fit_transform(data_pros)\n",
    "# totWords = len(TotalWords_vectorizer.get_feature_names())\n",
    "# tf_vectorizer = CountVectorizer(max_df = 0.90, min_df=0.01)\n",
    "# tf = tf_vectorizer.fit_transform(data_pros_cleaned)\n",
    "# tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# percVoc = len(tf_feature_names) / totWords * 100\n",
    "# percVoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the models in parallel and return a dictionary with the output of the get_models function for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bad_output = {}\n",
    "output_dictionary = {} # dictionary for the output\n",
    "\n",
    "for sparse_tup in vectorizers_list:\n",
    "    # to run get models in parallel, some parameters have to be fixed into a new function\n",
    "    # so we do that here with partial\n",
    "    partial_func = partial(get_models, tf=sparse_tup, tup_num=1)\n",
    "    try:\n",
    "        # you will run several topics at a time for each company\n",
    "        with cf.ProcessPoolExecutor() as e:\n",
    "            output = list(e.map(partial_func, our_range))\n",
    "        output_dictionary[sparse_tup[0]] = output\n",
    "    except:\n",
    "        bad_tup = vectorizers_list.pop(vectorizers_list.index(sparse_tup))\n",
    "        bad_output[bad_tup[0]] = bad_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now iterate over the output from above, add each dataset into a list, and then concatenate them all into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 ms, sys: 11.4 ms, total: 56.8 ms\n",
      "Wall time: 59.9 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hays</td>\n",
       "      <td>2</td>\n",
       "      <td>0.274745</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.238256</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hays</td>\n",
       "      <td>50</td>\n",
       "      <td>0.166055</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hays</td>\n",
       "      <td>100</td>\n",
       "      <td>0.151692</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hays</td>\n",
       "      <td>150</td>\n",
       "      <td>0.128355</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hays</td>\n",
       "      <td>200</td>\n",
       "      <td>0.118317</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hays</td>\n",
       "      <td>250</td>\n",
       "      <td>0.075295</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hays</td>\n",
       "      <td>300</td>\n",
       "      <td>0.067352</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2</td>\n",
       "      <td>0.204476</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>10</td>\n",
       "      <td>0.230088</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>50</td>\n",
       "      <td>0.238064</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>100</td>\n",
       "      <td>0.193579</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>150</td>\n",
       "      <td>0.150198</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>200</td>\n",
       "      <td>0.134041</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>250</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   company  topics  coherence  \\\n",
       "0                     Hays       2   0.274745   \n",
       "1                     Hays      10   0.238256   \n",
       "2                     Hays      50   0.166055   \n",
       "3                     Hays     100   0.151692   \n",
       "4                     Hays     150   0.128355   \n",
       "5                     Hays     200   0.118317   \n",
       "6                     Hays     250   0.075295   \n",
       "7                     Hays     300   0.067352   \n",
       "0  Boston Consulting Group       2   0.204476   \n",
       "1  Boston Consulting Group      10   0.230088   \n",
       "2  Boston Consulting Group      50   0.238064   \n",
       "3  Boston Consulting Group     100   0.193579   \n",
       "4  Boston Consulting Group     150   0.150198   \n",
       "5  Boston Consulting Group     200   0.134041   \n",
       "6  Boston Consulting Group     250   0.114120   \n",
       "\n",
       "                                              models  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  \n",
       "6  LatentDirichletAllocation(learning_method='onl...  \n",
       "7  LatentDirichletAllocation(learning_method='onl...  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  \n",
       "6  LatentDirichletAllocation(learning_method='onl...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_list = []\n",
    "for data in output_dictionary.keys(): # you can use the keys to get the data\n",
    "    temp_df = pd.DataFrame.from_dict(output_dictionary[data])\n",
    "    dfs_list.append(temp_df)\n",
    "\n",
    "output_dfs = pd.concat(dfs_list)\n",
    "output_dfs.columns = ['company', 'topics', 'coherence', 'models']\n",
    "output_dfs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates over the new dataframe, searches for the top 2 topics based on highest coherence, and appends to a list a tuple containing the company, a tuple with the top two topic numbers, and the fitted vectorizer from the original `vectorizers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_topics = []\n",
    "for comp, tup in zip(unique_ids, vectorizers_list):\n",
    "    condition = output_dfs['company'] == comp # get each company\n",
    "    the_data = output_dfs[condition] # get an exclusive dataset for a company\n",
    "    top_condition = the_data['coherence'].argsort() # get a sorted index based on coherence\n",
    "    top_topics = the_data.loc[top_condition, 'topics'].values # get the sorted topics based on coherence\n",
    "    sorted_topics.append((comp, (top_topics[-2], top_topics[-1]), tup[1])) # put all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `get_models` function again over the new space of topics. You will  need to\n",
    "1. sort the tuple with the top two topics.\n",
    "2. create a linearly spaced array with 10 elements between the top 2 topics, turn it into integers, make the array a set to eliminate any duplicates that might arise if there is a 2 in the top two topics, and then turn that into a list.\n",
    "3. get your fixed partial function again\n",
    "4. the output is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 4.13 s, total: 5.73 s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_dictionary2 = {}\n",
    "\n",
    "for tup in sorted_topics:\n",
    "    start, end = sorted(tup[1]) # since the top 2 topics might not be sorted, sort them first\n",
    "    the_range = list(set(np.linspace(start, end, 10).astype(int))) \n",
    "    partial_func = partial(get_models, tf=tup, tup_num=2)\n",
    "    with cf.ProcessPoolExecutor() as e:\n",
    "        output = list(e.map(partial_func, the_range))\n",
    "    output_dictionary2[tup[0]] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple dataframes from dictionaries again and collapse them into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.6 ms, sys: 11.9 ms, total: 70.5 ms\n",
      "Wall time: 72.6 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>topics</th>\n",
       "      <th>coherence</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hays</td>\n",
       "      <td>2</td>\n",
       "      <td>0.274745</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hays</td>\n",
       "      <td>3</td>\n",
       "      <td>0.265255</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hays</td>\n",
       "      <td>4</td>\n",
       "      <td>0.219521</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hays</td>\n",
       "      <td>5</td>\n",
       "      <td>0.243558</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hays</td>\n",
       "      <td>6</td>\n",
       "      <td>0.262954</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hays</td>\n",
       "      <td>7</td>\n",
       "      <td>0.249298</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hays</td>\n",
       "      <td>8</td>\n",
       "      <td>0.240442</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hays</td>\n",
       "      <td>9</td>\n",
       "      <td>0.219684</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hays</td>\n",
       "      <td>10</td>\n",
       "      <td>0.238256</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>32</td>\n",
       "      <td>0.224570</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>36</td>\n",
       "      <td>0.252745</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>41</td>\n",
       "      <td>0.247721</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>10</td>\n",
       "      <td>0.230088</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>45</td>\n",
       "      <td>0.231485</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>14</td>\n",
       "      <td>0.223990</td>\n",
       "      <td>LatentDirichletAllocation(learning_method='onl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   company  topics  coherence  \\\n",
       "0                     Hays       2   0.274745   \n",
       "1                     Hays       3   0.265255   \n",
       "2                     Hays       4   0.219521   \n",
       "3                     Hays       5   0.243558   \n",
       "4                     Hays       6   0.262954   \n",
       "5                     Hays       7   0.249298   \n",
       "6                     Hays       8   0.240442   \n",
       "7                     Hays       9   0.219684   \n",
       "8                     Hays      10   0.238256   \n",
       "0  Boston Consulting Group      32   0.224570   \n",
       "1  Boston Consulting Group      36   0.252745   \n",
       "2  Boston Consulting Group      41   0.247721   \n",
       "3  Boston Consulting Group      10   0.230088   \n",
       "4  Boston Consulting Group      45   0.231485   \n",
       "5  Boston Consulting Group      14   0.223990   \n",
       "\n",
       "                                              models  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  \n",
       "6  LatentDirichletAllocation(learning_method='onl...  \n",
       "7  LatentDirichletAllocation(learning_method='onl...  \n",
       "8  LatentDirichletAllocation(learning_method='onl...  \n",
       "0  LatentDirichletAllocation(learning_method='onl...  \n",
       "1  LatentDirichletAllocation(learning_method='onl...  \n",
       "2  LatentDirichletAllocation(learning_method='onl...  \n",
       "3  LatentDirichletAllocation(learning_method='onl...  \n",
       "4  LatentDirichletAllocation(learning_method='onl...  \n",
       "5  LatentDirichletAllocation(learning_method='onl...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_list = []\n",
    "for data in output_dictionary2.keys():\n",
    "    temp_df = pd.DataFrame.from_dict(output_dictionary2[data])\n",
    "    dfs_list.append(temp_df)\n",
    "\n",
    "output_dfs = pd.concat(dfs_list)\n",
    "output_dfs.columns = ['company', 'topics', 'coherence', 'models']\n",
    "output_dfs.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best topic based on the new output, and get the top 10 words per topic. At the moment, you are only adding 1 of the topics for each company but you can change this by removing the indexing in `top_topics` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 28.3 ms, total: 211 ms\n",
      "Wall time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_topics_model = defaultdict(tuple) # the output goes here\n",
    "\n",
    "for company, vrizer in zip(unique_ids, vectorizers_list):\n",
    "    cond = output_dfs['company'] == company # get each company\n",
    "    filtered_data = output_dfs[cond] # to get a single dataframe\n",
    "    the_topic = int(filtered_data.loc[filtered_data['coherence'].idxmax(), 'topics']) # get the best topic based on max coherence\n",
    "    the_model = filtered_data.loc[filtered_data['coherence'].idxmax(), 'models'] # get the best model based on max coherence\n",
    "    top_topics = show_topics(vrizer[2], the_model, 10) # get the top 10 words for each topic in each company\n",
    "    best_topics_model[company] = (the_topic, the_model, top_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(tuple,\n",
       "            {'Hays': (2,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=2, random_state=1234),\n",
       "              array(['de', 'que', 'la', 'el', 'lo', 'en', 'para', 'es', 'manag', 'te'],\n",
       "                    dtype='<U19')),\n",
       "             'Boston Consulting Group': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['onli', 'one', 'get', 'project', 'und', 'stori', 'reprimand',\n",
       "                     'divers', 'im', 'auf'], dtype='<U24')),\n",
       "             'Oracle': (20,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=20, random_state=1234),\n",
       "              array(['corpor', 'decis', 'process', 'sometim', 'feel', 'cog', 'strong',\n",
       "                     'rule', 'organis', 'understand'], dtype='<U21')),\n",
       "             'Philips': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['challeng', 'market', 'busi', 'africa', 'intern', 'health', 'good',\n",
       "                     'student', 'rare', 'break'], dtype='<U22')),\n",
       "             'IBM': (23,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=23, random_state=1234),\n",
       "              array(['slow', 'career', 'salari', 'opportun', 'lot', 'project',\n",
       "                     'process', 'need', 'mani', 'chang'], dtype='<U21')),\n",
       "             'Amazon': (14,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=14, random_state=1234),\n",
       "              array(['need', 'team', 'lot', 'compani', 'servic', 'salari', 'sometim',\n",
       "                     'bit', 'fast', 'manag'], dtype='<U21')),\n",
       "             'Orange': (10,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        random_state=1234),\n",
       "              array(['comfort', 'work', 'get', 'good', 'home', 'nt', 'provid', 'polici',\n",
       "                     'infrastructur', 'en'], dtype='<U16')),\n",
       "             'DXC Technology': (27,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=27, random_state=1234),\n",
       "              array(['even', 'promot', 'manag', 'constant', 'pay', 'de', 'employe',\n",
       "                     'rais', 'je', 'show'], dtype='<U24')),\n",
       "             'Deloitte': (72,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=72, random_state=1234),\n",
       "              array(['figur', 'existnat', 'everyth', 'almost', 'hr', 'remedi', 'result',\n",
       "                     'harder', 'profit', 'happen'], dtype='<U21')),\n",
       "             'Citi': (61,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=61, random_state=1234),\n",
       "              array(['manag', 'caus', 'environ', 'compani', 'analysi', 'bias', 'root',\n",
       "                     'call', 'centr', 'typic'], dtype='<U18')),\n",
       "             'Microsoft': (45,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=45, random_state=1234),\n",
       "              array(['la', 'que', 'en', 'insuficient', 'situacin', 'ao', 'realment',\n",
       "                     'un', 'se', 'inflacionaria'], dtype='<U29')),\n",
       "             'Altran Group': (2,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=2, random_state=1234),\n",
       "              array(['salari', 'manag', 'work', 'compani', 'nt', 'veri', 'peopl',\n",
       "                     'project', 'intern', 'employe'], dtype='<U16')),\n",
       "             'NTT': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['thi', 'er', 'je', 'de', 'none', 'includ', 'te', 'en', 'ze',\n",
       "                     'result'], dtype='<U21')),\n",
       "             'Continental': (2,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=2, random_state=1234),\n",
       "              array(['work', 'salari', 'manag', 'veri', 'compani', 'low', 'sometim',\n",
       "                     'und', 'get', 'high'], dtype='<U22')),\n",
       "             'Thales': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['que', 'lentrepris', 'il', 'sein', 'ridicul', 'scars', 'tant',\n",
       "                     'i3', 'huid', 'jeun'], dtype='<U16')),\n",
       "             'Thermo Fisher Scientific': (61,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=61, random_state=1234),\n",
       "              array(['lemon', 'emploe', 'unpaid', 'ver', 'high', 'speed', 'level',\n",
       "                     'task', 'empresari', 'underpaid'], dtype='<U19')),\n",
       "             'Google': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['easi', 'gettato', 'pu', 'busi', 'person', 'nuotar', 'ti', 'work',\n",
       "                     'vieni', 'devi'], dtype='<U15')),\n",
       "             'Nokia': (61,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=61, random_state=1234),\n",
       "              array(['intern', 'continu', 'competit', 'due', 'reduct', 'headcount',\n",
       "                     'employe', 'strong', 'talent', 'interact'], dtype='<U20')),\n",
       "             'Ericsson-Worldwide': (66,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=66, random_state=1234),\n",
       "              array(['student', 'alway', 'stay', 'salari', 'solut', 'stack', 'influenc',\n",
       "                     'role', 'inevit', 'di'], dtype='<U21')),\n",
       "             'Procter & Gamble': (20,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=20, random_state=1234),\n",
       "              array(['manag', 'intern', 'workload', 'mani', 'claim', 'express',\n",
       "                     'cultur', 'usual', 'sake', 'seemingli'], dtype='<U16')),\n",
       "             'ABB': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['poor', 'happi', 'focu', 'employe', 'organiz', 'se', 'il', 'di',\n",
       "                     'da', 'loro'], dtype='<U21')),\n",
       "             'Hewlett Packard Enterprise | HPE': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['lot', 'job', 'compani', 'secur', 'long', 'stop', 'hold', 'social',\n",
       "                     'chao', 'moral'], dtype='<U17')),\n",
       "             'Capgemini': (41,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=41, random_state=1234),\n",
       "              array(['work', 'project', 'client', 'time', 'nt', 'thi', 'finish',\n",
       "                     'develop', 'expect', 'day'], dtype='<U24')),\n",
       "             'VMware': (4,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=4, random_state=1234),\n",
       "              array(['veri', 'grow', 'lack', 'low', 'learn', 'salari', 'onli',\n",
       "                     'individu', 'encourag', 'hike'], dtype='<U17')),\n",
       "             'Dell Technologies': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['move', 'slow', 'bit', 'career', 'easi', 'growth', 'look',\n",
       "                     'countri', 'anoth', 'intern'], dtype='<U18')),\n",
       "             'Salesforce': (23,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=23, random_state=1234),\n",
       "              array(['product', 'cultur', 'compani', 'good', 'support', 'peopl', 'like',\n",
       "                     'leadership', 'growth', 'manag'], dtype='<U17')),\n",
       "             'Johnson & Johnson': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['contract', 'staff', 'onli', 'old', 'extend', 'offic', 'introvert',\n",
       "                     'budget', 'limit', 'contractor'], dtype='<U18')),\n",
       "             'Shell': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['want', 'path', 'career', 'thi', 'silo', 'top', 'term', 'never',\n",
       "                     'ladder', 'standard'], dtype='<U18')),\n",
       "             'IQVIA': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['merger', 'compani', 'wa', 'go', 'know', 'acquisit', 'thi',\n",
       "                     'branch', 'get', 'project'], dtype='<U18')),\n",
       "             'PwC': (27,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=27, random_state=1234),\n",
       "              array(['everyth', 'sometim', 'difficult', 'import', 'priorit', 'seem',\n",
       "                     'pay', 'busi', 'could', 'diet'], dtype='<U23')),\n",
       "             'Siemens': (14,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=14, random_state=1234),\n",
       "              array(['die', 'muss', 'zu', 'man', 'balanc', 'sein', 'benefit', 'best',\n",
       "                     'impari', 'verschafft'], dtype='<U17')),\n",
       "             'KPMG': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['nicht', 'viel', 'senior', 'arbeiten', 'manag', 'mit', 'anfang',\n",
       "                     'lavor', 'compens', 'one'], dtype='<U19')),\n",
       "             'Vodafone': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['salari', 'compani', 'limit', 'train', 'grow', 'wrong', 'insid',\n",
       "                     'process', 'posibl', 'di'], dtype='<U24')),\n",
       "             'EY': (27,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=27, random_state=1234),\n",
       "              array(['hour', 'balanc', 'work', 'long', 'worklif', 'achiev', 'usual',\n",
       "                     'longer', 'difficult', 'deadlin'], dtype='<U19')),\n",
       "             'Socit Gnrale': (32,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=32, random_state=1234),\n",
       "              array(['thi', 'top', 'slight', 'compar', 'noth', 'okay', 'peer', 'heavi',\n",
       "                     'disadvantag', 'salari'], dtype='<U13')),\n",
       "             'Amdocs': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['think', 'noth', 'post', 'posit', 'fact', 'requir', 'setup',\n",
       "                     'reason', 'cours', 'process'], dtype='<U19')),\n",
       "             'SAP': (27,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=27, random_state=1234),\n",
       "              array(['process', 'veri', 'sometim', 'flow', 'level', 'commun', 'easili',\n",
       "                     'play', 'effici', 'around'], dtype='<U18')),\n",
       "             'NCR': (61,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=61, random_state=1234),\n",
       "              array(['hay', 'capacitacin', 'de', 'que', 'por', 'manteniendos',\n",
       "                     'limitada', 'lo', 'sea', 'amiguismo'], dtype='<U18')),\n",
       "             'Luxoft': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['provid', 'client', 'resourc', 'especi', 'idea', 'fast', 'decis',\n",
       "                     'veri', 'appli', 'technic'], dtype='<U31')),\n",
       "             'Roche': (7,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=7, random_state=1234),\n",
       "              array(['ist', 'sometim', 'time', 'requir', 'die', 'result', 'reach',\n",
       "                     'grenzachwyhlen', 'freiburg', 'standortproblem'], dtype='<U23')),\n",
       "             'Verizon': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['evolut', 'blocag', 'dan', 'parfoi', 'de', 'le', 'middl', 'wide',\n",
       "                     'met', 'engin'], dtype='<U19')),\n",
       "             'Accenture': (61,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=61, random_state=1234),\n",
       "              array(['sono', 'new', 'person', 'ambient', 'frustrati', 'sentirsi',\n",
       "                     'compiti', 'molt', 'inizialment', 'questo'], dtype='<U17')),\n",
       "             'Cisco Systems': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['countri', 'job', 'get', 'home', 'cover', 'may', 'within',\n",
       "                     'especi', 'start', 'differ'], dtype='<U18')),\n",
       "             'Schneider Electric': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['disadvantag', 'integr', 'excit', 'consid', 'millenni', 'compani',\n",
       "                     'fix', 'poor', 'desk', 'post'], dtype='<U24')),\n",
       "             'HP Inc.': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['wa', 'direct', 'perk', 'unabl', 'among', 'visibl', 'futur',\n",
       "                     'vision', 'bad', 'pictur'], dtype='<U20')),\n",
       "             'Synopsys': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['clear', 'space', 'growth', 'traffic', 'quit', 'site', 'nt',\n",
       "                     'conveni', 'develop', 'individu'], dtype='<U18')),\n",
       "             'Mondelz International': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['endless', 'ad', 'cut', 'world', 'call', 'thi', 'faster',\n",
       "                     'constant', 'live', 'cost'], dtype='<U15')),\n",
       "             'Honeywell': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['time', 'compleat', 'task', 'work', 'activ', 'ask', 'stupid',\n",
       "                     'worri', 'write', 'use'], dtype='<U17')),\n",
       "             'Marriott International': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['one', 'work', 'handl', 'open', 'star', 'new', 'hotel', 'person',\n",
       "                     'nepal', 'low'], dtype='<U17')),\n",
       "             'Nielsen': (18,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=18, random_state=1234),\n",
       "              array(['low', 'pay', 'worklif', 'balanc', 'manag', 'micromanag',\n",
       "                     'supervisor', 'advncement', 'tough', 'bad'], dtype='<U21')),\n",
       "             'Intel Corporation': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['promot', 'amount', 'onc', 'time', 'constraint', 'contribut',\n",
       "                     'spent', 'imposs', 'bureaucraci', 'intel'], dtype='<U18')),\n",
       "             'Uber': (55,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=55, random_state=1234),\n",
       "              array(['job', 'thi', 'challeng', 'beyond', 'move', 'still', 'compani',\n",
       "                     'quiet', 'fast', 'everyday'], dtype='<U23')),\n",
       "             'McKinsey & Company': (14,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=14, random_state=1234),\n",
       "              array(['lifestyl', 'hour', 'long', 'travel', 'lot', 'get', 'work',\n",
       "                     'famili', 'make', 'could'], dtype='<U18')),\n",
       "             'Cognizant Technology Solutions': (50,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=50, random_state=1234),\n",
       "              array(['becaus', 'rob', 'may', 'traffic', 'lot', 'car', 'citi', 'spend',\n",
       "                     'flexibl', 'stuff'], dtype='<U14')),\n",
       "             'JLL': (45,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=45, random_state=1234),\n",
       "              array(['difficult', 'get', 'result', 'praktikanten', 'pay', 'bad',\n",
       "                     'turnov', 'cultur', 'sein', 'anspruchsvol'], dtype='<U18')),\n",
       "             'J.P. Morgan': (36,\n",
       "              LatentDirichletAllocation(learning_method='online', max_iter=100,\n",
       "                                        n_components=36, random_state=1234),\n",
       "              array(['much', 'veri', 'lot', 'like', 'wa', 'year', 'peopl', 'pretti',\n",
       "                     'code', 'feel'], dtype='<U22'))})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_topics_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the probabilities dataframes for each company and add them to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "for tup in vectorizers_list:\n",
    "    docs_of_probas[tup[0]] = pd.DataFrame(best_topics_model[tup[0]][1].transform(tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it with any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074576</td>\n",
       "      <td>0.925424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.414625</td>\n",
       "      <td>0.585375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131323</td>\n",
       "      <td>0.868677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015325</td>\n",
       "      <td>0.984675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064864</td>\n",
       "      <td>0.935136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.074576  0.925424\n",
       "1  0.414625  0.585375\n",
       "2  0.131323  0.868677\n",
       "3  0.015325  0.984675\n",
       "4  0.064864  0.935136"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_of_probas['Hays'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the measures of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 337 ms, total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comP_h_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comP_h_results[company] = comph(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Hays': 0.15636654768515945,\n",
       "             'Boston Consulting Group': 0.46648964955638167,\n",
       "             'Oracle': 0.478968100200936,\n",
       "             'Philips': 0.5726772280642697,\n",
       "             'IBM': 0.5139404811169749,\n",
       "             'Amazon': 0.33162896146287035,\n",
       "             'Orange': 0.48196172764276923,\n",
       "             'DXC Technology': 0.5419433639470068,\n",
       "             'Deloitte': 0.55674284690722,\n",
       "             'Citi': 0.5635069996183281,\n",
       "             'Microsoft': 0.5397208589996931,\n",
       "             'Altran Group': 0.23670762559293304,\n",
       "             'NTT': 0.5886421360935645,\n",
       "             'Continental': 0.05630794090764686,\n",
       "             'Thales': 0.6012950497663045,\n",
       "             'Thermo Fisher Scientific': 0.5917394824443586,\n",
       "             'Google': 0.5706160051317885,\n",
       "             'Nokia': 0.5749893790231159,\n",
       "             'Ericsson-Worldwide': 0.5749165685067223,\n",
       "             'Procter & Gamble': 0.5330230949281948,\n",
       "             'ABB': 0.5748935030627028,\n",
       "             'Hewlett Packard Enterprise | HPE': 0.5528211089157369,\n",
       "             'Capgemini': 0.5719394713896954,\n",
       "             'VMware': 0.29716859966596954,\n",
       "             'Dell Technologies': 0.5393441439123093,\n",
       "             'Salesforce': 0.54625227759552,\n",
       "             'Johnson & Johnson': 0.5749608167878747,\n",
       "             'Shell': 0.5902229299942153,\n",
       "             'IQVIA': 0.5752691323301853,\n",
       "             'PwC': 0.48730364450044134,\n",
       "             'Siemens': 0.4934292528268654,\n",
       "             'KPMG': 0.569871612992906,\n",
       "             'Vodafone': 0.5704696242187272,\n",
       "             'EY': 0.49570411352453136,\n",
       "             'Socit Gnrale': 0.5618329723748402,\n",
       "             'Amdocs': 0.5711777898243217,\n",
       "             'SAP': 0.5181093115542058,\n",
       "             'NCR': 0.5966465401999166,\n",
       "             'Luxoft': 0.5834757649860165,\n",
       "             'Roche': 0.40555468255858756,\n",
       "             'Verizon': 0.5882374096835068,\n",
       "             'Accenture': 0.5598614501503194,\n",
       "             'Cisco Systems': 0.567003037083669,\n",
       "             'Schneider Electric': 0.5721551228475845,\n",
       "             'HP Inc.': 0.5739671335855779,\n",
       "             'Synopsys': 0.5912261365878682,\n",
       "             'Mondelz International': 0.5932636327849196,\n",
       "             'Honeywell': 0.5639446223422193,\n",
       "             'Marriott International': 0.5517810392063178,\n",
       "             'Nielsen': 0.5266457314576887,\n",
       "             'Intel Corporation': 0.555310643297714,\n",
       "             'Uber': 0.586186290238835,\n",
       "             'McKinsey & Company': 0.5080840674024951,\n",
       "             'Cognizant Technology Solutions': 0.5768530191706247,\n",
       "             'JLL': 0.581316084250435,\n",
       "             'J.P. Morgan': 0.5622982020763474})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comP_h_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.9 ms, sys: 1.7 ms, total: 67.6 ms\n",
      "Wall time: 66.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "comT_h_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comT_h_results[company] = conth(proba_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Hays': 1.15990630696603,\n",
       "             'Boston Consulting Group': 1.5273851689351268,\n",
       "             'Oracle': 1.5290793170685046,\n",
       "             'Philips': 1.3521083775745097,\n",
       "             'IBM': 1.480713987271778,\n",
       "             'Amazon': 1.5031065497406941,\n",
       "             'Orange': 1.4141583176362618,\n",
       "             'DXC Technology': 1.3233398744456017,\n",
       "             'Deloitte': 1.5290347104782267,\n",
       "             'Citi': 1.397235559085598,\n",
       "             'Microsoft': 1.5557032540745293,\n",
       "             'Altran Group': 1.1383887181516652,\n",
       "             'NTT': 1.2821028386420261,\n",
       "             'Continental': 1.1503332294182536,\n",
       "             'Thales': 1.2368725879167042,\n",
       "             'Thermo Fisher Scientific': 1.3139249617242328,\n",
       "             'Google': 1.3848285769709197,\n",
       "             'Nokia': 1.3921235975819903,\n",
       "             'Ericsson-Worldwide': 1.3925109933961002,\n",
       "             'Procter & Gamble': 1.3516376642093169,\n",
       "             'ABB': 1.3285467519560543,\n",
       "             'Hewlett Packard Enterprise | HPE': 1.4258354022081194,\n",
       "             'Capgemini': 1.3383656344127246,\n",
       "             'VMware': 1.337317417368271,\n",
       "             'Dell Technologies': 1.4944398879258156,\n",
       "             'Salesforce': 1.3835930598901134,\n",
       "             'Johnson & Johnson': 1.3527570396669675,\n",
       "             'Shell': 1.2990042021607555,\n",
       "             'IQVIA': 1.3530180008228827,\n",
       "             'PwC': 1.5302598496831423,\n",
       "             'Siemens': 1.4304832055590315,\n",
       "             'KPMG': 1.294087403428617,\n",
       "             'Vodafone': 1.3912384865351073,\n",
       "             'EY': 1.6138169192097653,\n",
       "             'Socit Gnrale': 1.3387635266707258,\n",
       "             'Amdocs': 1.3598633045637594,\n",
       "             'SAP': 1.52326690166468,\n",
       "             'NCR': 1.274750341010794,\n",
       "             'Luxoft': 1.3414308354287103,\n",
       "             'Roche': 1.4484774322450937,\n",
       "             'Verizon': 1.2883390410639668,\n",
       "             'Accenture': 1.417212386102168,\n",
       "             'Cisco Systems': 1.4499695999749858,\n",
       "             'Schneider Electric': 1.3742209655344337,\n",
       "             'HP Inc.': 1.3562071299248333,\n",
       "             'Synopsys': 1.3087319477050054,\n",
       "             'Mondelz International': 1.2643848852256168,\n",
       "             'Honeywell': 1.4700097374935064,\n",
       "             'Marriott International': 1.428657780294682,\n",
       "             'Nielsen': 1.4557094982103078,\n",
       "             'Intel Corporation': 1.4004249738976415,\n",
       "             'Uber': 1.327430965811944,\n",
       "             'McKinsey & Company': 1.340123899833626,\n",
       "             'Cognizant Technology Solutions': 1.3450344754635846,\n",
       "             'JLL': 1.3224495946521728,\n",
       "             'J.P. Morgan': 1.3614769522137367})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comT_h_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_avg(probMatrix):\n",
    "    entropy_list = []\n",
    "    for i in range(len(probMatrix)): \n",
    "        entropy_list.append(sp.stats.entropy(probMatrix[i]))\n",
    "    return np.mean(entropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 2.17 ms, total: 147 ms\n",
      "Wall time: 147 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "entropy_avg_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    entropy_avg_results[company] = ent_avg(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Hays': 0.25135673394381847,\n",
       "             'Boston Consulting Group': 0.98287822415024,\n",
       "             'Oracle': 0.909744182722897,\n",
       "             'Philips': 0.9012621672696225,\n",
       "             'IBM': 0.9283546982620743,\n",
       "             'Amazon': 0.8260683717319083,\n",
       "             'Orange': 0.708551114672254,\n",
       "             'DXC Technology': 0.7686060257191989,\n",
       "             'Deloitte': 1.1169513340931527,\n",
       "             'Citi': 1.0123599254882198,\n",
       "             'Microsoft': 1.069389442006894,\n",
       "             'Altran Group': 0.2293772321887769,\n",
       "             'NTT': 0.7454709264978693,\n",
       "             'Continental': 0.24634447937508844,\n",
       "             'Thales': 0.7089317576354653,\n",
       "             'Thermo Fisher Scientific': 0.8965154993454862,\n",
       "             'Google': 0.9873047542527129,\n",
       "             'Nokia': 1.0012034203667712,\n",
       "             'Ericsson-Worldwide': 1.0119539909111064,\n",
       "             'Procter & Gamble': 0.7714543303000102,\n",
       "             'ABB': 0.8171575765752437,\n",
       "             'Hewlett Packard Enterprise | HPE': 0.9289163543727559,\n",
       "             'Capgemini': 0.8562807862571941,\n",
       "             'VMware': 0.5165043042904435,\n",
       "             'Dell Technologies': 0.9651765120562575,\n",
       "             'Salesforce': 0.8352323249410772,\n",
       "             'Johnson & Johnson': 0.9378561813273617,\n",
       "             'Shell': 0.8396167369849122,\n",
       "             'IQVIA': 0.8813300635270939,\n",
       "             'PwC': 0.9740355291048327,\n",
       "             'Siemens': 0.7980471840936738,\n",
       "             'KPMG': 0.7729891982904643,\n",
       "             'Vodafone': 0.9455612945459505,\n",
       "             'EY': 1.0440655083658763,\n",
       "             'Socit Gnrale': 0.8358446222994741,\n",
       "             'Amdocs': 0.9370258938480963,\n",
       "             'SAP': 0.9950648769203596,\n",
       "             'NCR': 0.8174816489131973,\n",
       "             'Luxoft': 0.9192664380744463,\n",
       "             'Roche': 0.6806732308696825,\n",
       "             'Verizon': 0.8219014836745671,\n",
       "             'Accenture': 1.0084991132313135,\n",
       "             'Cisco Systems': 1.0080469386888216,\n",
       "             'Schneider Electric': 0.8687458172891952,\n",
       "             'HP Inc.': 0.9742222054901674,\n",
       "             'Synopsys': 0.8377987934543484,\n",
       "             'Mondelz International': 0.7516466590052059,\n",
       "             'Honeywell': 1.0507690424203466,\n",
       "             'Marriott International': 1.0778539495446766,\n",
       "             'Nielsen': 0.8497071030835868,\n",
       "             'Intel Corporation': 0.8896221124349449,\n",
       "             'Uber': 0.8819078251707781,\n",
       "             'McKinsey & Company': 0.7040961136836411,\n",
       "             'Cognizant Technology Solutions': 0.9289759561549338,\n",
       "             'JLL': 0.8755812753556,\n",
       "             'J.P. Morgan': 0.9117171171278141})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the cross-entropy of two probability distributions\n",
    "def cross_entropy(p, q):\n",
    "    for i in range(len(p)):\n",
    "        p[i] = p[i]+1e-12\n",
    "    for i in range(len(q)):\n",
    "        q[i] = q[i]+1e-12\n",
    "\n",
    "    return -sum([p[i] * np.log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "# function to compute the average cross-entropy of a matrix\n",
    "def avg_crossEnt(probMatrix): \n",
    "#    NOTE: Cross entropy is not symmetric. \n",
    "#    This function takes both cross-entropy(p,q) and cross-entropy(q,p) \n",
    "#    into account when computing the avg\n",
    "    crossEntropy_list = []\n",
    "    for i in range(len(probMatrix)):\n",
    "        for j in range(len(probMatrix)): \n",
    "            if i != j:\n",
    "                crossEntropy_list.append(cross_entropy(probMatrix[i], probMatrix[j]))\n",
    "    return np.mean(crossEntropy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 470 ms, total: 3min 6s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cross_entropy_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    cross_entropy_results[company] = avg_crossEnt(proba_df.values)\n",
    "    \n",
    "# avg_crossEnt(docs_topics_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Hays': 1.6315595042873519,\n",
       "             'Boston Consulting Group': 6.855773551378029,\n",
       "             'Oracle': 6.369534734002661,\n",
       "             'Philips': 8.523173231911164,\n",
       "             'IBM': 6.880260728543732,\n",
       "             'Amazon': 4.446740058161019,\n",
       "             'Orange': 5.729308382583838,\n",
       "             'DXC Technology': 7.475667923235373,\n",
       "             'Deloitte': 8.774646798356992,\n",
       "             'Citi': 8.586383204505747,\n",
       "             'Microsoft': 7.988058224658032,\n",
       "             'Altran Group': 2.2218737902786168,\n",
       "             'NTT': 8.45915008216844,\n",
       "             'Continental': 0.8037882588572419,\n",
       "             'Thales': 8.95540857906868,\n",
       "             'Thermo Fisher Scientific': 9.047061148966662,\n",
       "             'Google': 8.576177298839772,\n",
       "             'Nokia': 8.769721655648048,\n",
       "             'Ericsson-Worldwide': 8.874126661607887,\n",
       "             'Procter & Gamble': 6.932245553702421,\n",
       "             'ABB': 8.13213977591657,\n",
       "             'Hewlett Packard Enterprise | HPE': 7.694249355502628,\n",
       "             'Capgemini': 8.288117311819997,\n",
       "             'VMware': 3.2124651473968697,\n",
       "             'Dell Technologies': 7.574036850362387,\n",
       "             'Salesforce': 7.298245346972492,\n",
       "             'Johnson & Johnson': 8.50847431763111,\n",
       "             'Shell': 8.758945990401326,\n",
       "             'IQVIA': 8.590101930000461,\n",
       "             'PwC': 6.757182454994317,\n",
       "             'Siemens': 6.12175614138086,\n",
       "             'KPMG': 7.904886847635889,\n",
       "             'Vodafone': 8.498428088061123,\n",
       "             'EY': 6.886466425775424,\n",
       "             'Socit Gnrale': 7.832994984466654,\n",
       "             'Amdocs': 8.461097123333653,\n",
       "             'SAP': 7.11791407546414,\n",
       "             'NCR': 9.11853149477304,\n",
       "             'Luxoft': 8.777074894824299,\n",
       "             'Roche': 4.686273169023864,\n",
       "             'Verizon': 8.758190726576114,\n",
       "             'Accenture': 8.601758513244805,\n",
       "             'Cisco Systems': 8.584558310189708,\n",
       "             'Schneider Electric': 8.144218627770922,\n",
       "             'HP Inc.': 8.595824399822254,\n",
       "             'Synopsys': 8.955939419771443,\n",
       "             'Mondelz International': 8.836860057213194,\n",
       "             'Honeywell': 8.383302456563852,\n",
       "             'Marriott International': 8.180461976491618,\n",
       "             'Nielsen': 6.785077870066283,\n",
       "             'Intel Corporation': 7.923048640693311,\n",
       "             'Uber': 8.848528057372784,\n",
       "             'McKinsey & Company': 6.295148939372631,\n",
       "             'Cognizant Technology Solutions': 8.546951458444902,\n",
       "             'JLL': 8.489925445090226,\n",
       "             'J.P. Morgan': 7.920367054430506})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
