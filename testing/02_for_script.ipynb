{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Culture Measures Based on Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "# nlkt.download('punkt')\n",
    "\n",
    "import koolture as kt\n",
    "import time\n",
    "\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean_gs.csv')\n",
    "\n",
    "## Fist Range of Topics\n",
    "\n",
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300\n",
    "\n",
    "comps_of_interest = df.employer.value_counts()\n",
    "\n",
    "comps_of_interest = (comps_of_interest[(comps_of_interest > 2)]).index\n",
    "\n",
    "cond2 = df['employer'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['employer'].unique() # get the unique IDs or unique employers in the dataset\n",
    "\n",
    "reviews_nums = df_interest['employer'].value_counts().reset_index()\n",
    "reviews_nums.columns = ['company', 'reviews_nums']\n",
    "\n",
    "## Fix Custom Stopwords List Before Cleaning\n",
    "\n",
    "data_pros = df_interest['pros'].values\n",
    "stopwords = nltk.corpus.stopwords.words('english') + [token.lower() for token in unique_ids]\n",
    "\n",
    "normalize_doc = partial(kt.normalize_doc, stopwords=stopwords)\n",
    "\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(kt.root_of_word, data_pros_cleaned))\n",
    "\n",
    "df_interest['pros_clean'] = data_pros_cleaned\n",
    "\n",
    "comp_ids = []\n",
    "i = 0\n",
    "\n",
    "for num in unique_ids:\n",
    "    another_list = list(unique_ids[i:int(i+1000)])\n",
    "    comp_ids.append(another_list)\n",
    "    i += 1000\n",
    "    \n",
    "chunks = list(filter(None, comp_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \"\"\"\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, comps_chunk in enumerate(chunks):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cond2 = df_interest['employer'].isin(comps_chunk) # create the condition\n",
    "    df_chunk = df_interest[cond2].copy() # get the new dataset\n",
    "    unique_ids = df_chunk['employer'].unique() # get the unique IDs or unique employers in the dataset\n",
    "\n",
    "    vectorizers_dicts = kt.get_vectorizers(data=df_chunk, unique_ids=unique_ids,\n",
    "                                      company_col='employer', reviews_col='pros', \n",
    "                                      vrizer=CountVectorizer())\n",
    "    \n",
    "    reviews_nums = df_chunk['employer'].value_counts().reset_index()\n",
    "    reviews_nums.columns = ['employerID', 'reviews_nums']\n",
    "    \n",
    "    partial_func = partial(kt.get_models, topics=our_range, \n",
    "                           vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "    with cf.ProcessPoolExecutor() as e:\n",
    "        output = list(e.map(partial_func, unique_ids))\n",
    "\n",
    "    output_df = kt.build_dataframe(output)\n",
    "\n",
    "    topics_sorted, comps, tops = kt.top_two_topics(data=output_df, companies_var='company',\n",
    "                                   coherence_var='coherence', topics_var='topics',\n",
    "                                   unique_ids=unique_ids, vrizers_list=vectorizers_dicts.values())\n",
    "\n",
    "    partial_func = partial(kt.get_models, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "    with cf.ProcessPoolExecutor() as e:\n",
    "        output2 = list(e.map(partial_func, comps, tops))\n",
    "\n",
    "    output_df2 = kt.build_dataframe(output2)\n",
    "\n",
    "    best_topics = kt.absolute_topics(output_df2, 'company', 'coherence', \n",
    "                                     'topics', 'models', vectorizers_dicts.values())\n",
    "\n",
    "\n",
    "    docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "    for tup in vectorizers_dicts.values():\n",
    "        docs_of_probas[tup[0]] = pd.DataFrame(best_topics[tup[0]][1].transform(tup[1]))\n",
    "\n",
    "    # Calculate the measures of interest\n",
    "\n",
    "    comP_h_results = defaultdict(float)\n",
    "    comT_h_results = defaultdict(float)\n",
    "    entropy_avg_results = defaultdict(float)\n",
    "    cross_entropy_results = defaultdict(float)\n",
    "\n",
    "    for company, proba_df in docs_of_probas.items():\n",
    "        comP_h_results[company] = kt.comph(proba_df.values)\n",
    "        comT_h_results[company] = kt.conth(proba_df)\n",
    "        entropy_avg_results[company] = kt.ent_avg(proba_df.values)\n",
    "        cross_entropy_results[company] = kt.avg_crossEnt(proba_df.values)\n",
    "\n",
    "    comph_df = pd.DataFrame.from_dict(comP_h_results.items())\n",
    "    conth_df = pd.DataFrame.from_dict(comT_h_results.items())\n",
    "    crossEnt_df = pd.DataFrame.from_dict(cross_entropy_results.items())\n",
    "    cultureMetrics = comph_df.merge(conth_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "    cultureMetrics = cultureMetrics.merge(crossEnt_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "    cultureMetrics.columns = ['employerID', 'comph', 'conth', 'avgCrossEnt']\n",
    "\n",
    "    df_best_topics = pd.DataFrame.from_records(best_topics).T.reset_index()\n",
    "    df_best_topics.columns = ['employerID', 'best_topic', 'model', 'coherence']\n",
    "\n",
    "    # df_best_topics.merge(reviews_nums, on='company', how='right').head()\n",
    "\n",
    "    (cultureMetrics.merge(reviews_nums, on='employerID', how='right')\n",
    "                   .merge(df_best_topics, on='employerID', how='right')\n",
    "                   .to_csv(f'culturemetrics_chunk_num_{i}.csv', index=False))\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Chunk # {i} just finished and it took: {str(elapsed_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
