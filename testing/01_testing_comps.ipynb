{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Culture Measures Based on Company Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk, re, math, csv\n",
    "# nltk.download('wordnet')\n",
    "# nlkt.download('punkt')\n",
    "\n",
    "import koolture as kt\n",
    "\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employer</th>\n",
       "      <th>id</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Express</td>\n",
       "      <td>44001</td>\n",
       "      <td>Still not big enough in market place</td>\n",
       "      <td>Great brand , Good leadership , Clear business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Nothing important on my point of view.</td>\n",
       "      <td>Learn new technologies, helpful people, good m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Alot of friends working together which isn't v...</td>\n",
       "      <td>Very good opportunities to learn technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>Working hours are not good and need to add the...</td>\n",
       "      <td>You can learn technically a lot in this company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eventum IT Solutions</td>\n",
       "      <td>44004</td>\n",
       "      <td>No Real Cons at all</td>\n",
       "      <td>- Very friendly environment.\\r\\n- Highly exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               employer     id  \\\n",
       "0      American Express  44001   \n",
       "1  Eventum IT Solutions  44004   \n",
       "2  Eventum IT Solutions  44004   \n",
       "3  Eventum IT Solutions  44004   \n",
       "4  Eventum IT Solutions  44004   \n",
       "\n",
       "                                                pros  \\\n",
       "0               Still not big enough in market place   \n",
       "1             Nothing important on my point of view.   \n",
       "2  Alot of friends working together which isn't v...   \n",
       "3  Working hours are not good and need to add the...   \n",
       "4                                No Real Cons at all   \n",
       "\n",
       "                                                cons  \n",
       "0  Great brand , Good leadership , Clear business...  \n",
       "1  Learn new technologies, helpful people, good m...  \n",
       "2      Very good opportunities to learn technologies  \n",
       "3   You can learn technically a lot in this company.  \n",
       "4  - Very friendly environment.\\r\\n- Highly exper...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean_gs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78420, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will remove the company names from their respective reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the root of the word. You can get all three (lemma, stem, and snow) or use them separately with the partial functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps with the preprocessing of the data. It runs after the lemmatizer, stemmer, snowball, etc. If you want to include stopwords and take them out at a later stage, uncomment the first `filtered_tokens` below and comment out the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to get the words forming top topics and to run the LDA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_range = 2, 10, 50, 100, 150, 200, 250, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array with the unique employers in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the company names from the reviews, and extract the reviews into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Amazon               561\n",
       "Oracle               422\n",
       "Microsoft            349\n",
       "Siemens              343\n",
       "Dell Technologies    338\n",
       "IBM                  337\n",
       "EY                   324\n",
       "PwC                  300\n",
       "Name: employer, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comps_of_interest = df.employer.value_counts()\n",
    "comps_of_interest.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985,\n",
       " Index(['Amazon', 'Oracle', 'Microsoft', 'Siemens', 'Dell Technologies', 'IBM',\n",
       "        'EY', 'PwC', 'SAP', 'Citi',\n",
       "        ...\n",
       "        'TalkTalk Group', 'ams', 'Ryanair', 'KBC', 'dormakaba', 'Huxley',\n",
       "        'Babylon Health', 'Systems Evolution', 'Maxar Technologies', 'BESIX'],\n",
       "       dtype='object', length=985))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comps_of_interest = (comps_of_interest).index\n",
    "comps_of_interest = (comps_of_interest[(comps_of_interest > 11)]).index\n",
    "len(comps_of_interest), comps_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = df['employer'].isin(comps_of_interest) # create the condition\n",
    "df_interest = df[cond2].copy() # get the new dataset\n",
    "unique_ids = df_interest['employer'].unique() # get the unique IDs or unique employers in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34929, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 22.5 ms, total: 3.76 s\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_interest = kt.comp_name_out(df_interest, 'employer', 'pros', comps_of_interest)\n",
    "data_pros = df_interest['pros'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text preprocessing of the corpus takes place in parallel. You first normalize the reviews and then take the root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 3.81 s, total: 21.8 s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    data_pros_cleaned = e.map(kt.normalize_doc, data_pros)\n",
    "    data_pros_cleaned = list(e.map(kt.root_of_word, data_pros_cleaned))\n",
    "\n",
    "df_interest['pros_clean'] = data_pros_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you create an array with all of the companies and the amount of reviews they have. So far, only companies with at least 2 reviews make it to the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the employers that meet the condition above by creating a boolean with True for yes and False for no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop will create sparse matrices for all companies and return a list of tuples with the name of the company, its sparse matrix, and the fitted vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 10.9 ms, total: 2.77 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizers_dicts = kt.get_vectorizers(data=df_interest, unique_ids=unique_ids,\n",
    "                                      company_col='employer', reviews_col='pros', \n",
    "                                      vrizer=CountVectorizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total words in the dictionary of review words, and get the percentage of words in the final dictionary that can be found in the full corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block run the models in parallel over the range of topics specified in our_range variable and return a dictionary with the output of the get_models function for each company. It is used to identify the interval to search further for optimal topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "partial_func = partial(kt.get_models, topics=our_range, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output = list(e.map(partial_func, unique_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will now iterate over the dictionary output from above, add each dataset into a list, and then concatenate them all into one dataset (output df contains exactly same information, but more readable, and used in next blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = kt.build_dataframe(output)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop iterates over the new dataframe, searches for the top 2 topics based on highest coherence, and appends to a list a tuple containing the company, a tuple with the top two topic numbers, and the fitted vectorizer from the original `vectorizers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topics_sorted, comps, tops = kt.top_two_topics(data=output_df, companies_var='company',\n",
    "                               coherence_var='coherence', topics_var='topics',\n",
    "                               unique_ids=unique_ids, vrizers_list=vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the `get_models` function again over the new space of topics. You will  need to\n",
    "1. sort the tuple with the top two topics.\n",
    "2. create a linearly spaced array with 10 elements between the top 2 topics, turn it into integers, make the array a set to eliminate any duplicates that might arise if there is a 2 in the top two topics, and then turn that into a list.\n",
    "3. get your fixed partial function again\n",
    "4. the output is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "partial_func = partial(kt.get_models, vrizer_dicts=vectorizers_dicts, unique_ids=unique_ids)\n",
    "\n",
    "with cf.ProcessPoolExecutor() as e:\n",
    "    output2 = list(e.map(partial_func, comps, tops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple dataframes from dictionaries again and collapse them into 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_df2 = kt.build_dataframe(output2)\n",
    "output_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best topic based on the new output, and get the top 10 words per topic. At the moment, you are only adding 1 of the topics for each company but you can change this by removing the indexing in `top_topics` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_topics = kt.absolute_topics(output_df2, 'company', 'coherence', \n",
    "                                 'topics', 'models', vectorizers_dicts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out your output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the probabilities dataframes for each company and add them to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate matrix summarizing distribution of docs (reviews) over topics\n",
    "docs_of_probas = defaultdict(pd.DataFrame)\n",
    "\n",
    "for tup in vectorizers_dicts.values():\n",
    "    docs_of_probas[tup[0]] = pd.DataFrame(best_topics[tup[0]][1].transform(tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the measures of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "comP_h_results = defaultdict(float)\n",
    "comT_h_results = defaultdict(float)\n",
    "entropy_avg_results = defaultdict(float)\n",
    "cross_entropy_results = defaultdict(float)\n",
    "\n",
    "for company, proba_df in docs_of_probas.items():\n",
    "    comP_h_results[company] = kt.comph(proba_df.values)\n",
    "    comT_h_results[company] = kt.conth(proba_df)\n",
    "    entropy_avg_results[company] = kt.ent_avg(proba_df.values)\n",
    "    cross_entropy_results[company] = kt.avg_crossEnt(proba_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comph_df = pd.DataFrame.from_dict(comP_h_results.items())\n",
    "conth_df = pd.DataFrame.from_dict(comT_h_results.items())\n",
    "crossEnt_df = pd.DataFrame.from_dict(cross_entropy_results.items())\n",
    "cultureMetrics = comph_df.merge(conth_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics = cultureMetrics.merge(crossEnt_df, how = 'inner', right_on = 0, left_on = 0)\n",
    "cultureMetrics.columns = ['employerID', 'comph', 'conth', 'avgCrossEnt']\n",
    "cultureMetrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureMetrics.to_csv('CultureMetrics_TestSample_1000.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
